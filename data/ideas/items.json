{
  "ideas": [
    {
      "id": "IDEA-001",
      "title": "Structured code annotations for codebase explorer",
      "description": "A doc-comment format the tool can parse (like JSDoc but for dev-track — @dt-description, @dt-depends, @dt-external) that feeds into the Codebase Explorer. AI writes these as it develops, tool parses them for rich tooltips and explanations. Not just basic commenting — structured metadata the tool understands.",
      "category": "feature",
      "status": "dismissed",
      "source": "conversation 2026-02-07 (ideas thread)",
      "related_ideas": [
        "IDEA-002",
        "IDEA-003"
      ],
      "promoted_to": null,
      "pros": [
        "Non-engineers can understand codebase via tooltips",
        "AI maintains them as it writes code",
        "Parseable structured data, not freeform comments"
      ],
      "cons": [
        "Adds noise to source files",
        "AI has to be disciplined about maintaining them",
        "Need to define a spec for the annotation format"
      ],
      "open_questions": [
        "What annotation format? JSDoc-like? Magic comments?",
        "How do we handle annotation drift when code changes?",
        "Do we parse on scan or in real-time?"
      ],
      "notes": "Requires the Codebase Explorer to be working and useful first so we know what annotations actually matter. Phase 3+. DISMISSED: Superseded by scanner-driven approach (IDEA-017, promoted to ai-watcher). Code IS the source of truth — no special comments needed.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-002",
      "title": "Initialization wizard with AI-powered project analysis",
      "description": "When dev-track is added to a new project, an initialization flow where the AI scans the codebase, infers the project structure, writes initial state.json (system ratings), captures TODOs/FIXMEs as backlog items, generates a context recovery briefing, and verifies the tool reflects the project status accurately.",
      "category": "feature",
      "status": "exploring",
      "source": "conversation 2026-02-07 (ideas thread)",
      "related_ideas": [
        "IDEA-003"
      ],
      "promoted_to": null,
      "pros": [
        "Zero-to-productive in one command",
        "AI does the heavy lifting of understanding the project",
        "Verifiable win condition"
      ],
      "cons": [
        "Quality depends on model capability",
        "Large codebases may need chunked analysis",
        "Without good existing docs, AI analysis is limited"
      ],
      "open_questions": [
        "How to handle projects with zero documentation?",
        "Multi-pass for smaller models — how to chunk?",
        "What's the minimum viable initialization?"
      ],
      "notes": "Weekly refresh: 14+ days old in captured status. Project init is critical for dogfooding on Landmark. Elevating to exploring to signal active consideration. Wizard should leverage docs generation system (already built) for initial project intelligence.",
      "created": "2026-02-07",
      "updated": "2026-02-09",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-003",
      "title": "Multi-model support with context-aware budgeting",
      "description": "Different AI models have different context windows and costs. dev-track should adapt: Opus with 1M context gets the full story, GPT-4o-mini gets a compressed version, smaller models get just the Quick Status + Now items.",
      "category": "architecture",
      "status": "dismissed",
      "source": "conversation 2026-02-07 (ideas thread)",
      "related_ideas": [
        "IDEA-002"
      ],
      "promoted_to": null,
      "pros": [
        "Works for everyone regardless of model/budget",
        "Prevents context overflow on smaller models",
        "Cost-conscious by default"
      ],
      "cons": [
        "Harder to test across all model sizes",
        "Compression may lose important nuance"
      ],
      "open_questions": [
        "Auto-detect model or manual config?",
        "Should we have preset profiles (heavy/medium/light)?"
      ],
      "notes": "The verbosity settings already exist in config. This extends them with model-aware profiles. DISMISSED: Verbosity settings (detailed/summary/minimal) already cover this use case. No need for separate model-aware budgeting.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-004",
      "title": "dev-track as an open-source product",
      "description": "Open-source the file format + cursor rule + basic UI. File format becoming a standard is more valuable than the tool itself.",
      "category": "business",
      "status": "rejected",
      "source": "conversation 2026-02-07 (ideas thread)",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [
        "Growing market of AI-assisted developers",
        "No existing tool solves this exact problem",
        "Open-source builds community and adoption"
      ],
      "cons": [
        "Small TAM right now",
        "Core file format is easily replicated",
        "Needs real usage validation first"
      ],
      "open_questions": [
        "When to extract? After how much dogfooding?",
        "License: MIT? Apache? AGPL?"
      ],
      "notes": "Nightly audit: Dismissed. BN-013 explicitly rejects open source. User said 'fuck open source I wanna get paid.' IDEA-021 (paid product distribution) supersedes this. Status was 'exploring' but the decision has been made.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-005",
      "title": "Codebase visualization with interactive diagrams",
      "description": "Interactive diagrams: dependency graphs (react-flow), page-to-API-to-DB flow charts, call trees. Click a node to see details. Hover for tooltips. Filter by module/system. Layman-friendly architecture explorer.",
      "category": "feature",
      "status": "promoted",
      "source": "conversation 2026-02-07 (ideas thread)",
      "related_ideas": [
        "IDEA-001",
        "IDEA-015",
        "IDEA-017"
      ],
      "promoted_to": "codebase-visualizer",
      "pros": [
        "Visual understanding of complex codebases",
        "Non-engineers can navigate architecture",
        "Impressive demo for open-source launch"
      ],
      "cons": [
        "react-flow adds complexity",
        "Layout algorithms for large graphs are hard"
      ],
      "open_questions": [],
      "notes": "BUILDING NOW. Using react-flow + dagre. Three views: module architecture, file dependencies, API route map.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-006",
      "title": "Always-on diagnostic triggers",
      "description": "Post-deploy hooks (Vercel webhook), Sentry webhook auto-create issues, cron-based health checks. Tool surfaces problems before the user even opens it.",
      "category": "architecture",
      "status": "parked",
      "source": "conversation 2026-02-07 (ideas thread)",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [
        "Catches problems automatically",
        "AI starts sessions with real-time awareness"
      ],
      "cons": [
        "Requires dev-track server running",
        "Webhook setup is project-specific"
      ],
      "open_questions": [
        "Local-only or need a cloud relay?",
        "Queue system needed?"
      ],
      "notes": "Phase 4+. Requires working integrations first. PARKED: Requires dev-track server running 24/7. Post-v1.0 feature.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-007",
      "title": "Background AI agents for R&D, docs, design iteration",
      "description": "Lightweight agents that run against dev-track data. R&D agent explores ideas. Docs agent keeps documentation in sync. Design agent iterates mockups. Run on triggers or schedules, not in the IDE.",
      "category": "architecture",
      "status": "parked",
      "source": "conversation 2026-02-07 (ideas thread)",
      "related_ideas": [
        "IDEA-006"
      ],
      "promoted_to": null,
      "pros": [
        "Work happens even when user isn't coding",
        "Separates concerns"
      ],
      "cons": [
        "Complex orchestration",
        "Cost if running frequently"
      ],
      "open_questions": [
        "Claude Code CLI as the agent runtime?",
        "How to trigger — schedule vs event?"
      ],
      "notes": "Far future. The data layer supports it — agents just read/write data/ files. PARKED: Far future architecture, not v0.1-v0.3 scope.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-008",
      "title": "Web-based multi-user version with concurrent sessions",
      "description": "Multiple users working concurrently with their own conversation threads. Sessions as first-class entities. API server handles serialization. Auth, permissions, activity attribution.",
      "category": "architecture",
      "status": "dismissed",
      "source": "conversation 2026-02-07 (session 2)",
      "related_ideas": [
        "IDEA-021",
        "IDEA-022",
        "IDEA-004"
      ],
      "promoted_to": null,
      "pros": [
        "Future-proofs for team usage",
        "API server already exists as coordination layer"
      ],
      "cons": [
        "Significant architectural lift",
        "File-based store may not scale"
      ],
      "open_questions": [
        "When does file-based storage hit its limit?",
        "How to handle auth?"
      ],
      "notes": "Weekly refresh: Web-based multi-user is far future (post-product launch). Current focus is single-user dogfooding. Multi-user prep (data/local/ separation) is done, but full web platform is years away. Dismissing to reduce noise.",
      "created": "2026-02-07",
      "updated": "2026-02-09",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-009",
      "title": "Integration data routing into native views",
      "description": "Instead of a flat integrations tab, route integration data into relevant views: Sentry errors flow into Issues as a separate section, Helicone costs into a Costs tab, Vercel deploys into a Deploys section on Dashboard. Each integration populates the view where its data is most useful.",
      "category": "feature",
      "status": "dismissed",
      "source": "conversation 2026-02-07 (session 3)",
      "related_ideas": [
        "IDEA-006",
        "IDEA-013",
        "IDEA-018"
      ],
      "promoted_to": null,
      "pros": [
        "Data appears where it's contextually relevant",
        "Reduces tab switching",
        "Makes integrations feel native not bolted-on"
      ],
      "cons": [
        "Complex routing logic per integration",
        "Need to handle missing integrations gracefully",
        "Each view needs integration-aware sections"
      ],
      "open_questions": [
        "How to handle when integration is down vs not configured?",
        "Do we need a separate Costs tab or just sections?"
      ],
      "notes": "Weekly refresh: Integration data routing is premature. Only 1/8 integrations working (ISS-003 open 14+ days). Should revisit after integrations are stable and tested. Dismissing for now.",
      "created": "2026-02-07",
      "updated": "2026-02-09",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-010",
      "title": "Global command-P style search across all data",
      "description": "Floating search bar at the top of every page (or hotkey-activated) that searches across backlog items, issues, ideas, changelog entries, codebase files, brain notes. Like Cursor's command palette but for dev-track data. Results grouped by type with quick-jump to the item.",
      "category": "ux",
      "status": "validated",
      "source": "conversation 2026-02-07 (session 3)",
      "related_ideas": [
        "IDEA-011",
        "IDEA-012",
        "IDEA-037"
      ],
      "promoted_to": null,
      "pros": [
        "Fast navigation",
        "Discoverability of old items",
        "Feels professional and powerful"
      ],
      "cons": [
        "Need a unified search index or multi-source search",
        "Performance if data grows large"
      ],
      "open_questions": [
        "Server-side search or client-side?",
        "Hotkey activation or always-visible?"
      ],
      "notes": "Weekly refresh: Global search is high-value UX improvement. Roadmap item 'global-search' exists in Next horizon. User repeatedly references Command-P pattern. Validating as ready for implementation.",
      "created": "2026-02-07",
      "updated": "2026-02-09",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-011",
      "title": "UI design overhaul — Cursor-inspired minimalism",
      "description": "Full design pass: replace emoji icons with clean SVG/Lucide icons, tighter spacing, better typography hierarchy, Cursor-inspired sidebar navigation. More professional, less playful. The goal is clean information density like Linear or Cursor's dashboard.",
      "category": "ux",
      "status": "validated",
      "source": "conversation 2026-02-07 (session 3)",
      "related_ideas": [
        "IDEA-010",
        "IDEA-012"
      ],
      "promoted_to": null,
      "pros": [
        "More professional appearance",
        "Better information density",
        "Consistent with tools developers already use"
      ],
      "cons": [
        "Significant effort across all 11 views",
        "Subjective — need to nail the aesthetic"
      ],
      "open_questions": [
        "Which icon set? Lucide is already installed.",
        "How far to deviate from current dark theme?"
      ],
      "notes": "Weekly refresh: UI design overhaul phase 2 is validated need. Roadmap item 'ui-design-overhaul' exists. Linear design benchmarking established (BN-024). Ready for implementation after dogfooding validates core functionality.",
      "created": "2026-02-07",
      "updated": "2026-02-09",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-012",
      "title": "Expanded descriptions and rich detail panels",
      "description": "Backlog items, issues, and ideas should support expanded descriptions — markdown-capable, with more context than a one-liner. Detail panels should show full context when clicked/expanded.",
      "category": "ux",
      "status": "captured",
      "source": "conversation 2026-02-07 (session 3)",
      "related_ideas": [
        "IDEA-010",
        "IDEA-011",
        "IDEA-019"
      ],
      "promoted_to": null,
      "pros": [
        "More context per item",
        "Reduces need to read raw JSON",
        "Better for handoffs"
      ],
      "cons": [
        "More data to manage",
        "UI gets busier if not done well"
      ],
      "open_questions": [
        "Markdown rendering or plain text?",
        "Inline expansion or slide-out panel?"
      ],
      "notes": "react-markdown is already in package.json. Could render descriptions as markdown in detail panels.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-013",
      "title": "Costs and observability dashboard",
      "description": "Cross-integration cost monitoring: Helicone for AI costs, Vercel for compute/bandwidth, Supabase for DB usage, Cursor spending. A unified costs view showing burn rate across all services.",
      "category": "feature",
      "status": "captured",
      "source": "conversation 2026-02-07 (session 3)",
      "related_ideas": [
        "IDEA-009",
        "IDEA-018",
        "IDEA-062"
      ],
      "promoted_to": null,
      "pros": [
        "Single pane of glass for all costs",
        "Could alert on spending spikes",
        "Useful for budget-conscious developers"
      ],
      "cons": [
        "Every provider has different billing APIs",
        "Some don't expose cost data via API"
      ],
      "open_questions": [
        "Which providers expose cost APIs?",
        "Manual entry fallback for those that don't?"
      ],
      "notes": "User showed $733 Cursor spend in screenshot. Cost awareness is clearly important to them.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-014",
      "title": "Background AI watcher — file-change-driven intelligence loop",
      "description": "A lightweight AI process that monitors the codebase for file changes in real-time. On change: analyze what changed, auto-update changelog, detect if backlog/issues/state need updating, and do it. Doesn't write code — just observes and tracks. Could use cheap models (GPT-4o-mini, Haiku) since it's analysis not generation. Runs alongside the coding AI, communicates via dev-track data files. The coding AI in Cursor/Claude leaves notes, the watcher AI picks them up and maintains the tracking system.",
      "category": "architecture",
      "status": "dismissed",
      "source": "conversation 2026-02-07 (session 3 — meta discussion)",
      "related_ideas": [
        "IDEA-007",
        "IDEA-006",
        "IDEA-017"
      ],
      "promoted_to": null,
      "pros": [
        "Removes human-in-the-loop for tracking",
        "Removes coding-AI attention burden for tracking",
        "Cheap models sufficient for analysis-only work",
        "File watcher already exists (chokidar)",
        "Data layer is the communication protocol between AIs",
        "Could stream-analyze changes as they come in"
      ],
      "cons": [
        "Cost adds up if triggered on every save",
        "Needs debouncing — 20 file changes in one prompt shouldn't trigger 20 analyses",
        "Needs to understand 'when the AI is done' vs mid-edit",
        "Context window still needed to understand changes meaningfully"
      ],
      "open_questions": [
        "What model? GPT-4o-mini? Haiku? Local model?",
        "Trigger on git commit? On file save with debounce? On session end?",
        "How does the coding AI signal 'I just finished a task' to the watcher?",
        "Can we use git diff as the input rather than raw file watching?"
      ],
      "notes": "Weekly refresh: Background AI watcher superseded by tiered audit system (IDEA-055 promoted to roadmap). Semantic file watcher (ISS-034) addresses the core need. Dismissing in favor of more focused approach.",
      "created": "2026-02-07",
      "updated": "2026-02-09",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-015",
      "title": "Layman-friendly module descriptions and relationship explanations",
      "description": "The architecture graph currently shows technical labels (Server, Server Routes) with no explanation of what they are or why they connect. A layman sees 'Server' and doesn't know it's a Hono HTTP server handling API requests. Each module needs a plain-English description auto-generated from its contents. Edges need explanations beyond 'imports X' — they should say 'Server Routes handles API requests and stores data using the Data Store.' Think Wikipedia summary, not JSDoc.",
      "category": "ux",
      "status": "captured",
      "source": "conversation 2026-02-07 (session 3 — graph feedback)",
      "related_ideas": [
        "IDEA-005",
        "IDEA-001",
        "IDEA-017"
      ],
      "promoted_to": null,
      "pros": [
        "Makes the graph actually useful for non-developers",
        "Differentiates dev-track from every other code visualization tool",
        "AI-generated descriptions can be very good"
      ],
      "cons": [
        "Auto-generation quality varies",
        "Needs to stay in sync as code changes",
        "Long descriptions clutter the graph"
      ],
      "open_questions": [
        "Generate at scan time or on-demand?",
        "Where to show: on the node, in the panel, or both?",
        "How to handle modules where auto-description would be vague?"
      ],
      "notes": "The scanner has all the data — file types, exports, external services, DB ops. We can generate descriptions like: 'This module handles all HTTP API requests. It has 16 route files covering backlog, issues, sessions, metrics, and more. It depends on the Data Store for persistence and broadcasts changes via WebSocket.' The detail panel should show this prominently.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-016",
      "title": "Living project wiki with auto-generated documentation",
      "description": "The Docs tab should be a full end-to-end wiki and source of truth for whatever project you're building. It should auto-update when architecture changes: new modules get documented, API changes get reflected, component libraries get cataloged. Think of it as a self-maintaining knowledge base that could onboard a new AI or developer to the project. Phase 1 (markdown rendering, ToC, nav) is built. Phase 2 needs: auto-doc generation from codebase scans, cross-references between docs and code, search within docs, and richer navigation with nested sections.",
      "category": "feature",
      "status": "exploring",
      "source": "conversation 2026-02-07 (session 4)",
      "related_ideas": [
        "IDEA-001",
        "IDEA-015"
      ],
      "promoted_to": "docs-wiki-overhaul",
      "pros": [
        "Single source of truth for the project",
        "Auto-updates reduce manual maintenance",
        "Onboarding tool for new AIs and developers",
        "Beautiful rendered markdown instead of raw files"
      ],
      "cons": [
        "Auto-generation quality may vary",
        "Need to handle multiple doc formats",
        "Could get stale if auto-update breaks"
      ],
      "open_questions": [
        "Should docs be markdown or HTML?",
        "How to auto-detect when architecture changed enough to trigger doc update?",
        "How to handle docs for external project (not dev-track itself)?"
      ],
      "notes": "User specifically wants: nested tabs/nav, table of contents with sections within sections, beautiful rendering, real-time updates on architecture changes. Look at Pillar project docs for reference on structure. Phase 1 shipped: react-markdown + remark-gfm rendering, file sidebar, auto-generated ToC, styled components for all markdown elements.",
      "created": "2026-02-07",
      "updated": "2026-02-07",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-017",
      "title": "Scanner-driven code understanding — no special comments needed",
      "description": "Instead of requiring the coding AI to write special annotations (@dt-description etc.), make the scanner smart enough to understand code from its existing structure. The scanner already has file types, exports, imports, function signatures, external services, and DB operations. From this, it can generate plain-English descriptions. The background watcher (IDEA-014) should analyze git diffs holistically (full commits) rather than individual file changes, so it gets context. This sidesteps the 'who writes the comments' and 'different comment syntax per language' problems entirely.",
      "category": "architecture",
      "status": "promoted",
      "source": "conversation 2026-02-07 (session 4 — architecture discussion)",
      "related_ideas": [
        "IDEA-001",
        "IDEA-014",
        "IDEA-015"
      ],
      "promoted_to": "ai-watcher",
      "pros": [
        "No code pollution with special comments",
        "Works across all languages",
        "Code IS the source of truth",
        "Already proven — module descriptions work well",
        "Background watcher gets context from git diffs not file saves"
      ],
      "cons": [
        "Auto-generated descriptions can be generic for unusual modules",
        "Requires good heuristics per module type"
      ],
      "open_questions": [
        "Should we add optional manual override for auto-descriptions?",
        "How to handle projects with unusual structures?"
      ],
      "notes": "Nightly audit: This idea (scanner-driven code understanding) is validated and effectively promoted. The approach has been implemented in the codebase scanner (generateModuleDescription, generateEdgeLabel). The background watcher aspect maps to the 'ai-watcher' backlog item. Marking as promoted.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-018",
      "title": "Helicone deep integration — sessions, properties, users, cache",
      "description": "Go beyond basic proxy: use Helicone Sessions to group multi-turn conversations, Properties for tagging by feature/task type, Users for per-user cost attribution, and Cache for identical prompt deduplication. Admin-level dashboard showing cost by feature, by user, by model. Critical for both dogfooding and eventual multi-tenant billing.",
      "category": "integration",
      "status": "captured",
      "source": "conversation 2026-02-07 (session 4)",
      "related_ideas": [
        "IDEA-013",
        "IDEA-009"
      ],
      "promoted_to": null,
      "pros": [
        "Granular cost visibility",
        "Caching saves money on repeated prompts",
        "Per-user tracking essential for teams",
        "Audit trail for AI decisions"
      ],
      "cons": [
        "Adds Helicone as a hard dependency for analytics",
        "Configuration complexity"
      ],
      "open_questions": [
        "Should we build our own analytics or rely on Helicone?",
        "How to handle when Helicone is down?"
      ],
      "notes": "User set up BYOK in Helicone. Ready to integrate deeply.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-019",
      "title": "Rich tool call tray — Pillar-style expandable elements with navigation",
      "description": "The chat tool calls should render as rich, interactive elements. Click a backlog item result to navigate to the Backlog view with that item highlighted. Click an issue to jump to Issues. Show clear icons per tool type, expandable results with formatted data, and persistent chat context while navigating. Pull from Pillar's tray system implementation.",
      "category": "ux",
      "status": "promoted",
      "source": "conversation 2026-02-07 (session 4)",
      "related_ideas": [
        "IDEA-012",
        "IDEA-037"
      ],
      "promoted_to": "rich-tool-tray",
      "pros": [
        "Makes tool calls actionable not just informational",
        "Chat becomes a navigation hub",
        "Feels like a real agent not just a chatbot"
      ],
      "cons": [
        "Significant UI complexity",
        "Need to coordinate chat state with view navigation"
      ],
      "open_questions": [
        "How to handle navigation while chat is mid-stream?",
        "Which tools get rich rendering vs plain text?"
      ],
      "notes": "Reference: Pillar's SidebarChat tray system and Landmark's ToolCallDisplay component.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-020",
      "title": "Self-hosted logging for dev-track-on-dev-track (vindaloop)",
      "description": "dev-track building dev-track. All AI calls, tool executions, errors, and performance data logged internally. A dedicated Logs view in the UI showing request/response pairs, latencies, costs, errors. This data feeds back into the dev-track AI so it can help debug itself. The ultimate dogfooding loop.",
      "category": "architecture",
      "status": "dismissed",
      "source": "conversation 2026-02-07 (session 4)",
      "related_ideas": [
        "IDEA-014",
        "IDEA-018"
      ],
      "promoted_to": null,
      "pros": [
        "Eat your own dogfood at every level",
        "AI can help debug AI issues",
        "Proves the tool works on complex real projects"
      ],
      "cons": [
        "Recursive complexity",
        "Log volume could get large"
      ],
      "open_questions": [
        "Store logs in data/ai/logs/ or a separate system?",
        "How much context does the AI need about its own logs?"
      ],
      "notes": "Weekly refresh: Self-hosted logging (vindaloop) is unnecessary complexity. Helicone integration exists and works (despite ISS-003 test failures). Audit system provides full transparency. Dismissing.",
      "created": "2026-02-07",
      "updated": "2026-02-09",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-021",
      "title": "Distribution model — lightweight local daemon + web platform",
      "description": "How to make dev-track a paid product. The local tool (daemon) handles: file watching, git monitoring, codebase scanning, context generation. The web platform handles: UI, AI chat, team features, billing, cross-project dashboards, admin. Daemon syncs to web via API. Web works without daemon (reduced features) or with it (full power). npm install for try-it, web platform for paid experience.",
      "category": "business",
      "status": "exploring",
      "source": "conversation 2026-02-07 (session 4)",
      "related_ideas": [
        "IDEA-008",
        "IDEA-022"
      ],
      "promoted_to": null,
      "pros": [
        "Preserves local-first real-time advantage",
        "Web handles auth/billing/teams",
        "npm distribution for easy trial",
        "Proven model (Cursor, Copilot, Linear)"
      ],
      "cons": [
        "Building both a daemon AND a web platform",
        "Sync reliability between local and cloud",
        "Offline mode complexity"
      ],
      "open_questions": [
        "Electron/Tauri desktop app vs pure daemon?",
        "Free tier scope?",
        "How to handle multi-project views?",
        "GitHub App integration as alternative entry point?"
      ],
      "notes": "See detailed analysis in session 4 conversation. Multiple distribution options explored: desktop app, CLI, IDE extension, GitHub App, hybrid daemon+web. Hybrid wins because it preserves what makes dev-track unique (local real-time context) while enabling what requires cloud (teams, billing, AI budget).",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-022",
      "title": "Users, teams, auth, billing, multi-tenant RLS",
      "description": "The full multi-user stack: user accounts, team workspaces, role-based access, Stripe billing, row-level security for multi-tenant data. Required for paid product but NOT required for v0.1 dogfooding. Backlog this until the core product is validated.",
      "category": "architecture",
      "status": "captured",
      "source": "conversation 2026-02-07 (session 4)",
      "related_ideas": [
        "IDEA-008",
        "IDEA-021"
      ],
      "promoted_to": null,
      "pros": [
        "Required for revenue",
        "Teams is a force multiplier for the product"
      ],
      "cons": [
        "Massive engineering scope",
        "Premature if core product isn't validated"
      ],
      "open_questions": [
        "Supabase Auth or custom?",
        "Stripe or Lemon Squeezy?",
        "When to start — after how much dogfooding?"
      ],
      "notes": "User correctly identified this as a can of worms. Park until v0.2+.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-023",
      "title": "Visual state capture — automated screenshots of every view for AI context",
      "description": "Dev-track automatically takes screenshots of each view/tab and maintains a visual state library. The AI can reference what the UI actually looks like without the user manually screenshotting. Could use Puppeteer/Playwright headless to capture each route. Stored in data/screenshots/ with timestamps. The chat agent can say 'here's what the dashboard looks like right now.' For development: instant visual feedback loop — AI sees what it built. For product: visual diffs between sessions, UI regression detection, onboarding screenshots.",
      "category": "feature",
      "status": "parked",
      "source": "conversation 2026-02-07 (session 4)",
      "related_ideas": [
        "IDEA-020"
      ],
      "promoted_to": null,
      "pros": [
        "AI gets visual context without user effort",
        "Visual diff detection between versions",
        "Great for AI-assisted UI iteration",
        "Kills the constant screenshot-and-paste workflow"
      ],
      "cons": [
        "Puppeteer/Playwright adds a heavy dependency",
        "Screenshots need to be taken at the right time",
        "Storage grows with every capture"
      ],
      "open_questions": [
        "Headless browser or actual browser automation?",
        "Trigger: on scan, on demand, on page change?",
        "How to efficiently diff screenshots?",
        "Should the chat be able to request a fresh screenshot?"
      ],
      "notes": "User currently does this manually constantly — screenshots to send to AI. Automating it is obvious. Could start simple: just capture on codebase scan and store the images. PARKED: Good idea but Puppeteer is heavy dependency. Not pursuing until post-validation.",
      "created": "2026-02-07",
      "updated": "2026-02-08",
      "priority": "P2",
      "tags": []
    },
    {
      "id": "IDEA-024",
      "title": "Session lifecycle system — clear start/end with AI-driven closure",
      "status": "promoted",
      "category": "core",
      "priority": "high",
      "description": "Every coding session needs a clear start and end. When a user starts a new chat window, the AI recognizes it's a new session and logs it in DevTrack. The AI is trained to push users toward session closure at the right time — especially users who overload context windows. Based on user profile (behavior patterns, context window habits), the AI prompts: 'Good stopping point. Let's push, update dev-track, and pick up in a fresh session.' Loads user profile at session start so any AI picking up knows how to handle this specific user.",
      "pros": [
        "Prevents context window overload",
        "Creates clean session boundaries for tracking",
        "Trains users on good habits",
        "User profile makes every AI session-aware"
      ],
      "cons": [
        "Users might find it annoying if pushed too aggressively",
        "Hard to detect 'right time' to prompt closure"
      ],
      "open_questions": [
        "What triggers session start detection?",
        "How aggressive should closure prompting be?",
        "Should there be a hard context limit that forces a new session?",
        "How does this work across different AI platforms (Cursor, Claude, etc)?"
      ],
      "notes": "Nightly audit: Already has a corresponding backlog item 'session-lifecycle'. Marking as promoted.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": [],
      "promoted_to": "session-lifecycle",
      "related_ideas": [
        "IDEA-025",
        "IDEA-026",
        "IDEA-054",
        "IDEA-029"
      ]
    },
    {
      "id": "IDEA-025",
      "title": "Session-end self-audit — DevTrack audits its own data freshness",
      "status": "promoted",
      "category": "core",
      "priority": "high",
      "description": "At the end of every session, DevTrack should audit itself: go through backlog, actions, issues, ideas, codebase, session notes, changelog, docs. Flag anything stale, contradicting, or out of date. Automatically clean up superseded brain notes, update statuses, archive old data. The system should be self-healing — if the AI missed a changelog entry or left an issue open that was fixed, the audit catches it.",
      "pros": [
        "Self-healing data",
        "Catches AI drift automatically",
        "Reduces manual maintenance",
        "Ensures dashboard always reflects reality"
      ],
      "cons": [
        "Could be expensive if using premium AI models",
        "Might make incorrect changes if context is incomplete"
      ],
      "open_questions": [
        "Which model tier for audit (budget should be fine)?",
        "How deep should the audit go?",
        "Should it make changes automatically or just flag them?",
        "Run at session end only, or periodically?"
      ],
      "notes": "Nightly audit: Already has a corresponding backlog item 'session-self-audit'. Automation engine now built with nightly-audit seeded. Marking as promoted.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": [],
      "promoted_to": "session-self-audit",
      "related_ideas": [
        "IDEA-024",
        "IDEA-047",
        "IDEA-054"
      ]
    },
    {
      "id": "IDEA-026",
      "title": "User behavior profiles — experience levels, habits, interaction patterns",
      "status": "promoted",
      "category": "core",
      "priority": "high",
      "description": "Build rich user profiles that go beyond basic info. Include: experience levels as a web diagram (frontend, backend, devops, AI, etc.), intelligence/capability ratings, behavioral patterns (context window habits, session length preferences, communication style), and how the AI should handle this user (push to closure? let them run? explain more? explain less?). Profile is loaded at every session start and injected into AI system prompts.",
      "pros": [
        "Every AI knows how to work with this user",
        "Personalized experience",
        "Better session management",
        "Could power team features later"
      ],
      "cons": [
        "Privacy concerns for team features",
        "Profiles might become stale",
        "Hard to auto-detect behavior patterns initially"
      ],
      "open_questions": [
        "Self-reported vs AI-observed profile data?",
        "How to visualize the web diagram in UI?",
        "Should profiles be editable by the user?",
        "How granular should behavior tracking be?"
      ],
      "notes": "Nightly audit: User profiles are SHIPPED. IQ scoring, radar charts, deep assessment, session observations, AI-to-AI guidance — all built. Marking as promoted to 'user-profiles' (completed).",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": [],
      "promoted_to": "user-profiles",
      "related_ideas": [
        "IDEA-024",
        "IDEA-029",
        "IDEA-030",
        "IDEA-040"
      ]
    },
    {
      "id": "IDEA-027",
      "title": "AI conversation bridge — extension/CLI that passes logs from any AI tool to DevTrack",
      "status": "exploring",
      "category": "core",
      "priority": "critical",
      "description": "A lightweight extension/CLI daemon that watches AI conversations in Cursor, Claude, Gemini, etc. Captures user messages + AI final response messages (not intermediate file changes). Passes logs to DevTrack's backend AI for lightweight parsing. Extracts: issues reported, decisions made, ideas captured, action items. Also detects session start (new chat window) and session end (wrap-up language or inactivity). The backend AI can then create issues, update backlog, write brain notes — filling in what the coding AI missed. This is the bridge that makes DevTrack truly intelligent across all AI platforms.",
      "pros": [
        "Solves ISS-006 structurally — no more relying on coding AI to self-track",
        "Works across any AI platform",
        "Lightweight — only captures messages, not file diffs",
        "Backend AI does the heavy lifting with cheap models",
        "Enables automatic session lifecycle detection"
      ],
      "cons": [
        "Extension development per platform (Cursor, VS Code, web)",
        "Privacy considerations — user conversations contain sensitive code",
        "AI cost for parsing every conversation",
        "Needs to handle different conversation formats per platform"
      ],
      "open_questions": [
        "Cursor extension vs CLI file watcher vs both?",
        "How to detect conversation boundaries in different tools?",
        "Should it capture in real-time or batch at intervals?",
        "How much context does the parsing AI need — just messages or also file change summaries?"
      ],
      "notes": "Nightly audit #2 (Feb 9): Still exploring at critical priority. No progress on implementation. This remains the most architecturally important idea — it solves ISS-006 and ISS-012 structurally. Server-side infrastructure exists (automation engine, headless runner). The capture mechanism (extension/CLI) is the missing piece. Should be actively explored once AI chat agent is validated. Estimated effort: L-XL (2-3 sessions). Could start with a simple approach: CLI command that reads Cursor's conversation log files (if accessible) and feeds them to the DevTrack AI for parsing.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": []
    },
    {
      "id": "IDEA-028",
      "title": "Deepgram voice integration — talk to DevTrack instead of typing",
      "status": "dismissed",
      "category": "feature",
      "priority": "medium",
      "description": "Integrate Deepgram (or similar) for voice-to-text in the DevTrack AI chat. User can talk to DevTrack instead of typing. Useful for brain dumps, session planning, quick updates. The DevTrack chat sidebar would have a mic button. Voice transcription feeds into the same chat pipeline.",
      "pros": [
        "Faster for brain dumps and planning",
        "User explicitly wants this — 'sometimes I type, sometimes I talk'",
        "Deepgram has good real-time transcription APIs",
        "Natural for non-technical users"
      ],
      "cons": [
        "Transcription quality varies (user noted 'self-rate' → 'self-rape')",
        "Additional API cost",
        "Needs good error handling for misheard words",
        "Audio privacy considerations"
      ],
      "open_questions": [
        "Deepgram vs Whisper vs browser native?",
        "Real-time streaming or record-then-transcribe?",
        "How to handle transcription errors gracefully?",
        "Should we use AI to clean up transcription before feeding to chat?"
      ],
      "notes": "Weekly refresh: Deepgram voice integration is far future feature. Current focus is dogfooding core functionality. Dismissing to reduce noise.",
      "created": "2026-02-08",
      "updated": "2026-02-09",
      "tags": []
    },
    {
      "id": "IDEA-029",
      "title": "Session-end user observation pipeline — AI-to-AI profile crowdsourcing",
      "status": "captured",
      "category": "core",
      "priority": "high",
      "description": "At the end of every coding session, the AI writes a brief observation about the user back to the DevTrack profile (session_observations array). Includes: attribute signals observed during the session, behavioral patterns, emotional tone, specific examples. DevTrack's backend AI periodically consolidates these observations into weighted scores — attributes that get reinforced across many sessions carry more weight than one-off signals. Essentially crowdsourced AI observations about the user, aggregated into a living profile.",
      "pros": [
        "Profile improves automatically over time",
        "Multiple AI perspectives (Cursor, Claude, Gemini) contribute",
        "Weighted scoring smooths out noise from single sessions",
        "Creates longitudinal data about user growth"
      ],
      "cons": [
        "Adds overhead to session wrap",
        "Different AIs may assess differently",
        "Need to handle conflicting observations"
      ],
      "open_questions": [
        "How often does DevTrack AI consolidate?",
        "How to weight observations from different platforms/models?",
        "Should the user be able to see raw observations?",
        "Minimum observations before a score adjustment?"
      ],
      "notes": "User explicitly asked for this. The session_observations structure is already in the profile. Now needs: (1) cursor rule instruction to write observation at session end, (2) DevTrack AI consolidation job, (3) weighted score recalculation logic.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": [],
      "related_ideas": [
        "IDEA-026",
        "IDEA-030",
        "IDEA-039"
      ]
    },
    {
      "id": "IDEA-030",
      "title": "Portable user intelligence profile — travels with the user across projects and tools",
      "status": "captured",
      "category": "business",
      "priority": "high",
      "description": "The user profile becomes a portable document that follows the user across projects, AI tools, and potentially employers/teams. Contains: AI-observed intelligence scores, cognitive strengths/weaknesses, work style, behavioral patterns, communication calibration. Could be: (1) exported as a JSON/PDF for sharing, (2) hosted on a user's dev-track web profile, (3) injected into any AI tool's system prompt. Use cases beyond dev-track: team composition (pair a high-vision/low-implementation person with a high-implementation partner), hiring (honest AI assessment vs self-reported resume), work product direction (assign architecture work to systems thinkers, testing work to detail-oriented people).",
      "pros": [
        "Makes every AI interaction smarter from message one",
        "Valuable for teams and hiring",
        "Differentiated product feature no one else has",
        "Natural extension of what we've already built"
      ],
      "cons": [
        "Privacy concerns are massive — honest AI assessments of intelligence/weaknesses",
        "Users might game the system if they know how scoring works",
        "Employers using this could be discriminatory",
        "Needs robust consent/sharing model"
      ],
      "open_questions": [
        "Who controls sharing? User opt-in only?",
        "How to prevent gaming?",
        "Is this ethical for hiring?",
        "Should there be a 'public' vs 'private' profile?"
      ],
      "notes": "User's insight: 'how interesting would this data be to employers if you were ACTUALLY honest.' The answer is: extremely. But also extremely sensitive. The product opportunity is there but the ethics need to be right. Start with: portable across the user's own projects and AI tools. Later: opt-in sharing for teams.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": [],
      "related_ideas": [
        "IDEA-026",
        "IDEA-029",
        "IDEA-040"
      ]
    },
    {
      "id": "IDEA-031",
      "title": "Encrypted key transfer for auto-populating credentials across projects",
      "status": "captured",
      "category": "security",
      "priority": "high",
      "description": "When initializing dev-track in a new project, credentials from .credentials.json auto-populate in the UI (already works locally). For the paid product, if we ever sync credentials between projects or from a central store, the transfer must be encrypted. HTTPS + encrypted storage at rest minimum. Consider: (1) per-project encryption key derived from a master password, (2) OS keychain integration (macOS Keychain, Windows Credential Manager), (3) zero-knowledge architecture where the server never sees plaintext keys. For local-only use this is fine — keys never leave the machine. Becomes critical when cloud sync or team features are added.",
      "pros": [
        "Enables secure cross-project credential sharing",
        "Users don't need to re-enter keys per project",
        "OS keychain integration is the gold standard for local credential storage"
      ],
      "cons": [
        "Adds complexity to a local-first tool",
        "OS keychain APIs differ per platform",
        "Master password UX is annoying"
      ],
      "open_questions": [
        "When does this become critical — only when cloud sync ships?",
        "OS keychain vs encrypted file vs master password?",
        "Should credentials ever sync to the web platform or stay local-only?"
      ],
      "notes": "User noticed credentials auto-populating from .credentials.json and flagged security concern proactively. Good instinct. Park until cloud sync is on the roadmap, but document the requirement now.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": []
    },
    {
      "id": "IDEA-032",
      "title": "AI project initialization wizard — full codebase audit on first run",
      "status": "validated",
      "category": "core",
      "priority": "critical",
      "description": "When dev-track init runs on a new project, the AI should audit the entire codebase end-to-end: parse all source files, find docs/READMEs, read package.json/configs, scan git commit history for changelog, look for existing sprint plans/product plans/TODOs/FIXMEs. It synthesizes everything into dev-track: populates state.json with system ratings, creates backlog items from TODOs and weak areas, creates issues from known bugs and security concerns, captures ideas, rates architectural health, identifies what's half-baked vs production-ready. Should work on mid-stream projects like Pillar and Landmark — not just greenfield.",
      "pros": [
        "Zero-to-productive in one command",
        "Works on existing projects mid-stream",
        "AI finds things humans miss (security gaps, incomplete features)",
        "Git history becomes changelog automatically",
        "Differentiating feature — no other tool does this"
      ],
      "cons": [
        "Expensive — full codebase + git history analysis",
        "Quality depends on AI model capability",
        "Large codebases may need chunked analysis",
        "Risk of generating noise (too many low-value issues)"
      ],
      "open_questions": [
        "Which model tier? Needs to be smart enough for architecture assessment",
        "How to handle monorepos vs single projects?",
        "Should it ask the user to confirm before populating, or just do it?",
        "How deep into git history — last 50 commits? All?"
      ],
      "notes": "Nightly audit #2 (Feb 9): Still validated at critical priority. The init-wizard backlog item exists but has narrower scope than this idea. IDEA-032 adds full codebase audit, security analysis, git history parsing, TODO/FIXME extraction. The POST /api/v1/init endpoint has been built (commit ca10e5e) with scanner + git + AI agent — this idea is partially implemented. The remaining gap is: the init endpoint exists but hasn't been tested on a real project (only Pillar was initialized via CLI, not the full AI-powered audit). Consider promoting to a separate backlog item 'ai-powered-init-audit' that extends init-wizard, or updating init-wizard's scope to include this. Deferring promotion until chat agent is validated — the init endpoint depends on the headless runner which depends on working AI.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": []
    },
    {
      "id": "IDEA-033",
      "title": "GitHub commit sync — parse git history into DevTrack changelog and context",
      "status": "captured",
      "category": "integration",
      "priority": "high",
      "description": "Sync git commit history directly into DevTrack. Parse commits into changelog entries, detect patterns (feature work, bug fixes, refactors). GitHub integration already exists — extend it to pull commit history on init and keep it synced. AI can then search DevTrack's own data instead of re-parsing git every time. Enables: 'what changed last week', 'when was auth last touched', 'show me all commits related to the payment system'.",
      "pros": [
        "Rich project history without manual entry",
        "AI can search structured commit data",
        "Connects code changes to backlog/issues automatically",
        "GitHub integration already has git CLI access"
      ],
      "cons": [
        "Commit messages vary wildly in quality",
        "Large repos = lots of data",
        "Need to handle merge commits, squashes, rebases"
      ],
      "open_questions": [
        "Parse on init only or keep syncing?",
        "How to map commits to backlog items/issues?",
        "Store raw commits or AI-summarized versions?"
      ],
      "notes": "User wants: load all previous git commits so AI can search them within DevTrack. GitHub integration already uses local git CLI.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": []
    },
    {
      "id": "IDEA-034",
      "title": "Product management audit — clarify Actions vs Backlog vs Issues vs Ideas relationships",
      "status": "exploring",
      "category": "architecture",
      "priority": "medium",
      "description": "The data model needs an audit. Current entities: Backlog (features/tasks with horizons), Issues (bugs with severity), Ideas (captured concepts), Actions (tracked features with health/diagnostics), Sessions, Changelog, Brain Notes. Questions: Are Actions redundant with Backlog? Should Issues auto-create from Backlog items? Should Ideas promote directly to Backlog? Is this Notion + Jira + GitHub? What's the relationship graph between entities? User explicitly wants clarity on what each entity IS and how they flow into each other. This is the product definition of DevTrack itself.",
      "pros": [
        "Cleaner mental model for users",
        "Reduces confusion about where to put things",
        "Enables better AI automation (knows the flow)",
        "Forces us to define what DevTrack IS as a product"
      ],
      "cons": [
        "May require data model changes",
        "Migration of existing data",
        "Risk of over-engineering the taxonomy"
      ],
      "open_questions": [
        "Should Actions be merged into Backlog?",
        "What's the lifecycle: Idea → Backlog → Issue? Or are they independent?",
        "Do we need all these entities or can some be collapsed?",
        "What does 'Notion + Jira + GitHub driven by AI' actually mean in entity terms?"
      ],
      "notes": "Nightly audit #2 (Feb 9): Entity Model v2 shipped, which resolved the core entity confusion. The 14-entity model provides clear separation. However, the 'Actions' entity question noted in this idea persists — the Actions entity type exists in the type system but its relationship to Roadmap items is unclear. Are Actions being used? The AI tools have an actions module (server/ai/tools/actions.ts) but the entity may be redundant with roadmap items. This idea is partially resolved but the remaining product definition question should be addressed. Downgrading from validated to exploring — the urgent part (entity model) is done, the remaining part (Actions cleanup) is a smaller concern.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": []
    },
    {
      "id": "IDEA-035",
      "title": "Opinionated git/commit settings page — AI behavior controls for version control",
      "status": "captured",
      "category": "feature",
      "priority": "high",
      "description": "Settings page section for git behavior. Conventional commit format (feat/fix/chore/etc), commit frequency preferences, push-on-session-end toggle, changelog generation from commits, how AI should handle branching. Settings inject into cursor rules so the coding AI follows them. Sliders/toggles for: auto-commit frequency, commit message verbosity, branch strategy, push policy, changelog linkage.",
      "pros": [
        "Opinionated defaults that teach good git practices",
        "AI follows user preferences consistently",
        "Conventional commits enable auto-changelog from git"
      ],
      "cons": [
        "Many git workflows to support",
        "Settings can conflict with team policies"
      ],
      "open_questions": [
        "How to inject settings into cursor rules dynamically?",
        "Default to conventional commits or let user choose format?"
      ],
      "notes": "User wants this to opinionate the coding AI's git behavior via cursor rule injection. Good product feature — teaches users proper git etiquette while they use the tool.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": []
    },
    {
      "id": "IDEA-036",
      "title": "Version management + release notes bundling",
      "status": "captured",
      "category": "feature",
      "priority": "medium",
      "description": "Track project versions (semver), bundle changelog entries into release notes per version, manage version bumps (major/minor/patch). AI can auto-suggest version bumps based on changelog types (feat = minor, fix = patch, breaking = major). Release notes page showing version history with bundled changes.",
      "pros": [
        "Professional release management",
        "AI auto-suggests version bumps",
        "Release notes auto-generated from changelog"
      ],
      "cons": [
        "Adds complexity for solo devs",
        "Version management is opinionated"
      ],
      "open_questions": [
        "When to auto-bump vs manual?",
        "How does this interact with npm/package.json versions?"
      ],
      "notes": "User called out need for version management and release notes on major releases.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": []
    },
    {
      "id": "IDEA-037",
      "title": "AI-unique views — displays that only exist because AI can generate them",
      "status": "exploring",
      "category": "feature",
      "priority": "critical",
      "description": "Purpose-built views that are impossible without AI: real-time system health inference (not manual ratings), predictive velocity ('payment system ships in ~4 sessions'), knowledge graph across all entities, vector search ('what did we decide about auth?'), risk heat maps, dependency impact analysis ('if we change this, what breaks?'), auto-generated architecture narratives, technical debt scoring, team capability matching. These are the views that make DevTrack impossible to replicate with human-only tools. Consider: vector DB (Pinecone/Weaviate), Neo4j for knowledge graph, GraphQL for cross-entity queries.",
      "pros": [
        "Killer differentiator",
        "Impossible without AI",
        "Makes DevTrack irreplaceable"
      ],
      "cons": [
        "Expensive to build and run",
        "Quality depends on model capability",
        "Infrastructure heavy (vector DB, graph DB)"
      ],
      "open_questions": [
        "Which AI-unique views are most valuable first?",
        "Vector DB vs simple search?",
        "Neo4j vs simpler graph representation?"
      ],
      "notes": "Nightly audit #2 (Feb 9): Still exploring at critical priority. AI-unique views remain the key differentiator. The headless runner + automation engine provide infrastructure to power these views. However, none of the prerequisite systems (chat agent, headless runner) have been validated yet. This idea should stay at exploring until the AI infrastructure is proven. First candidate views to build: (1) predictive velocity ('payment system ships in ~4 sessions'), (2) risk heat map, (3) knowledge graph across entities. These could be built as dashboard widgets first before full views.",
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "tags": [],
      "related_ideas": [
        "IDEA-010",
        "IDEA-019"
      ]
    },
    {
      "id": "IDEA-038",
      "title": "Project init must preserve existing cursor/editor settings",
      "description": "When dev-track init runs on a project that already has .cursor/rules, cursor settings, or other AI platform configs, the agent must: (1) read and retain existing settings, (2) not override or conflict with them, (3) merge dev-track rules alongside existing rules, (4) factor existing settings into its understanding of the project. Dev-track delivers its own cursor rules but should respect what is already there. The init agent needs read access to .cursor/ directory and project-level settings.",
      "category": "process",
      "status": "captured",
      "priority": "P1",
      "source": "conversation session-7",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [],
      "cons": [],
      "open_questions": [],
      "notes": null,
      "tags": [
        "init",
        "settings"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-039",
      "title": "Cron-based user profile iteration from ongoing conversations",
      "description": "Add a scheduled automation that monitors recent AI conversations (from dev-track chat, and eventually from the conversation bridge) and iterates on the user profile. Look for: new skill demonstrations, behavioral pattern changes, preference signals, communication style shifts. Update cognitive/technical scores, add session observations, refine AI-to-AI guidance. This makes the profile a living document that improves with every interaction, not just a static snapshot.",
      "category": "feature",
      "status": "captured",
      "priority": "P1",
      "source": "conversation session-7",
      "related_ideas": [
        "IDEA-029",
        "IDEA-026"
      ],
      "promoted_to": null,
      "pros": [],
      "cons": [],
      "open_questions": [],
      "notes": null,
      "tags": [
        "profiles",
        "automation",
        "persona"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-040",
      "title": "Infer developer persona from codebase and git history on init",
      "description": "During project initialization, the AI agent should infer the developer persona from: git commit style and frequency, code conventions, README writing style, dependency choices, architecture patterns. For single-person teams, analyze specific user commits deeply. Populates user profile with initial cognitive/technical scores and behavioral patterns. Used to instruct both dev-track AI and project cursor rule.",
      "category": "feature",
      "status": "captured",
      "priority": "P1",
      "source": "conversation session-7",
      "related_ideas": [
        "IDEA-026",
        "IDEA-030"
      ],
      "promoted_to": null,
      "pros": [
        "Immediate personalization from day 1",
        "No manual profile setup needed",
        "Code style analysis is objective data"
      ],
      "cons": [
        "Quality depends on git history depth",
        "Multi-contributor repos need disambiguation",
        "Privacy considerations if analyzing personal patterns"
      ],
      "open_questions": [
        "How deep to analyze (last 50 commits? all time?)",
        "How to handle repos with multiple contributors?"
      ],
      "notes": null,
      "tags": [
        "init",
        "profiles",
        "persona"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-041",
      "title": "Settings UI for automation controls (kill switch, model tier, budget, cooldown)",
      "description": "Add Automations section to Settings AI Configuration tab with: master on/off toggle, scheduler toggle, triggers toggle, model tier selector (premium/standard/budget), cooldown minutes, daily budget limit with current spend display. All reading from and writing to ai/config.json automations block. Must also show list of all automations with individual toggle controls, their trigger types, and last fired timestamps.",
      "category": "feature",
      "status": "promoted",
      "priority": "P1",
      "source": "conversation session-7",
      "related_ideas": [],
      "promoted_to": "automation-settings-ui",
      "pros": [],
      "cons": [],
      "open_questions": [],
      "notes": "Session 8: Promoted to roadmap. This is ISS-021. User can't control any automation settings without editing JSON directly.",
      "tags": [
        "ui",
        "settings",
        "automations"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-042",
      "title": "Brain note deduplication - prevent duplicate weekly reports and audit summaries",
      "description": "The automation engine ran the weekly report 3 times due to a race condition, creating BN-015, BN-018, BN-019 as duplicate weekly reports. Need dedup logic: either (a) check if a similar brain note was created in the last N hours before creating a new one, or (b) the automation prompt should list recent brain notes and avoid duplicating.",
      "category": "process",
      "status": "promoted",
      "priority": "P2",
      "source": "conversation session-7",
      "related_ideas": [
        "IDEA-046",
        "IDEA-053"
      ],
      "promoted_to": "brain-note-dedup",
      "pros": [],
      "cons": [],
      "open_questions": [],
      "notes": "Session 8: Promoted to roadmap. Blocking condition for re-enabling automations safely.",
      "tags": [
        "automations",
        "brain",
        "dedup"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-043",
      "title": "Entity provenance — three-label attribution system (User, DevTrack AI, External)",
      "description": "Every entity mutation should record who made the change using three actor labels: (1) User (specific person, e.g. 'Cole') — when the user directly edits in the DevTrack UI or via API, (2) DevTrack AI — when DevTrack's own AI (chat agent, automations, headless runner) makes changes, (3) External — when changes are detected from outside (code changes in background, git commits, file system changes). Default to External if no label is provided. The UI should show this attribution on every entity (badges, edit log). The chat service and automation engine already know when they're making changes — just need to propagate actor info through to tool execution. Include model ID when AI-made (e.g., 'claude-opus-4-6'). This is the foundation for the activity log UI.",
      "category": "feature",
      "status": "promoted",
      "priority": "P1",
      "source": "conversation session-7",
      "related_ideas": [
        "IDEA-044",
        "IDEA-045",
        "IDEA-060",
        "IDEA-061"
      ],
      "promoted_to": null,
      "pros": [
        "Full audit trail",
        "Know exactly what AI changed vs human",
        "Debugging AI behavior becomes possible",
        "Required for trust in autonomous AI operations"
      ],
      "cons": [
        "Touches every entity type and every mutation path",
        "Adds a field to every write operation"
      ],
      "open_questions": [
        "Do we store full edit history or just last_modified_by?",
        "Where does Cursor/external AI fit in the actor taxonomy?"
      ],
      "notes": "Session 8: Updated from 4-label to 3-label system per user decision. Promoted to roadmap. User: 'If it's a user directly editing it, mark as that user. If DevTrack AI edits it, mark as DevTrack AI. If it's external, mark as external. If no label, it's external.'",
      "tags": [
        "provenance",
        "audit",
        "trust"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-044",
      "title": "Full edit history log on all entities — who changed what, when, which model",
      "description": "Every entity should have an edit history: an array of { timestamp, actor, model, field, old_value, new_value } entries. Similar to Landmark's edit log system. Stored either inline on the entity or in a separate audit log file (data/audit/log.json). The UI shows this as an expandable \"History\" section on each entity detail view. Critical for: debugging AI behavior, understanding how data evolved, reverting bad AI edits, compliance.",
      "category": "feature",
      "status": "captured",
      "priority": "P1",
      "source": "conversation session-7",
      "related_ideas": [
        "IDEA-043",
        "IDEA-045",
        "IDEA-060"
      ],
      "promoted_to": null,
      "pros": [
        "Complete audit trail",
        "Can revert bad AI edits",
        "Understand data evolution",
        "Required for enterprise trust"
      ],
      "cons": [
        "Storage grows with every edit",
        "Need archival strategy for old history",
        "Adds latency to every write"
      ],
      "open_questions": [
        "Inline on entity vs separate audit log?",
        "How far back to keep history?",
        "Do we store full old/new values or just diffs?"
      ],
      "notes": null,
      "tags": [
        "audit",
        "history",
        "provenance"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-045",
      "title": "UI badges showing AI model attribution on created/edited entities",
      "description": "In the DevTrack UI, any entity that was created or last modified by AI should show a small badge: \"Claude Opus 4.6\" or \"GPT-5.2\" or \"AI Automation\" with the model name. This appears on roadmap items, issues, ideas, brain notes, docs, changelog entries — anywhere an entity is displayed. Color-coded by provider (Anthropic blue, OpenAI green, Google orange). Clicking the badge shows the edit history for that entity.",
      "category": "ux",
      "status": "captured",
      "priority": "P2",
      "source": "conversation session-7",
      "related_ideas": [
        "IDEA-043",
        "IDEA-044"
      ],
      "promoted_to": null,
      "pros": [
        "Immediately visible who/what made changes",
        "Builds trust in AI operations",
        "Beautiful UX detail"
      ],
      "cons": [
        "Need provenance data first (IDEA-043)",
        "Badge visual design needs care to not clutter"
      ],
      "open_questions": [
        "Badge design — icon? text? color dot?",
        "Show on list views or only detail views?"
      ],
      "notes": null,
      "tags": [
        "ui",
        "provenance",
        "badges"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-046",
      "title": "Headless runner conversation logging — save full AI transcripts from automations",
      "description": "The headless AI runner (server/ai/runner.ts) currently discards the full conversation after execution. Only tool call summaries are logged to the activity feed. Need to save full transcripts (system prompt, user message, all AI responses, all tool calls with full arguments and results) to data/ai/conversations/ just like ChatService does. Critical for: debugging automation behavior, auditing what the AI decided and why, cost attribution per conversation.",
      "category": "feature",
      "status": "promoted",
      "priority": "P1",
      "source": "conversation session-7",
      "related_ideas": [
        "IDEA-042",
        "IDEA-047",
        "IDEA-060"
      ],
      "promoted_to": null,
      "pros": [
        "Full audit trail for automations",
        "Debug why AI made specific decisions",
        "Cost attribution per automation run"
      ],
      "cons": [
        "Transcripts can be large (49 tool calls = big JSON)",
        "Need archival for old transcripts"
      ],
      "open_questions": [
        "Same conversation format as chat? Or separate?",
        "How long to keep automation transcripts?"
      ],
      "notes": "Session 8: Promoted to roadmap. Can't debug automations or attribute costs without full transcripts.",
      "tags": [
        "automations",
        "logging",
        "audit"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-047",
      "title": "Chat conversation audit automation — AI reviews its own conversations for insights",
      "description": "Add a scheduled automation that reads recent AI conversations (from data/ai/conversations/) and extracts: decisions made, ideas surfaced, issues discovered, preferences expressed, behavioral patterns observed. Updates the relevant DevTrack entities (ideas, issues, brain notes, user profile). This is the self-awareness loop — the AI reviews what it discussed and makes sure nothing fell through the cracks. Also works as the foundation for the conversation bridge (ISS-012) when external conversations are captured.",
      "category": "feature",
      "status": "captured",
      "priority": "P2",
      "source": "conversation session-7",
      "related_ideas": [
        "IDEA-046",
        "IDEA-025",
        "IDEA-051"
      ],
      "promoted_to": null,
      "pros": [
        "Nothing falls through the cracks",
        "Self-improving data capture",
        "Foundation for conversation bridge"
      ],
      "cons": [
        "Reading full transcripts is token-expensive",
        "Need to avoid infinite loops (AI auditing its own audit)"
      ],
      "open_questions": [
        "How often to audit? Daily? Per-conversation?",
        "How to prevent the audit from creating noise?"
      ],
      "notes": null,
      "tags": [
        "automations",
        "conversations",
        "self-awareness"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-048",
      "title": "AI-powered autonomous chaos testing — self-directed stress testing with virtual environments",
      "description": "An automated testing system where AI runs DevTrack headlessly in isolated virtual environments and invents its own test scenarios. The AI acts like a user: creates projects, adds backlog items, triggers automations, simulates file changes, tests edge cases, and reports back. Not traditional integration tests with fixed scripts — the AI **devises new situations** based on use cases and tries to break things.\n\n**How it works:**\n1. User sets constraints: max cost, max iterations, max time, number of parallel environments\n2. AI spins up N virtual DevTrack instances (Docker containers or separate data directories)\n3. AI generates test scenarios: \"What if a user creates 500 backlog items at once?\", \"What if automations trigger recursively?\", \"What if the user switches projects mid-conversation?\"\n4. AI executes scenarios via the headless runner and API calls\n5. AI observes: errors logged, performance metrics, data corruption, race conditions\n6. AI reports findings: issues discovered, edge cases hit, performance bottlenecks\n\n**Three modes:**\n- **Observer mode**: AI reports findings but doesn't edit code (safest)\n- **Assisted mode**: AI proposes fixes, user reviews before applying\n- **Autonomous mode**: AI fixes bugs it finds and commits (full self-healing)\n\n**Use cases:**\n- Stress testing before dogfooding on real projects\n- Continuous chaos testing in CI/CD\n- Discovering edge cases humans wouldn't think of\n- Load testing (100 concurrent chat sessions)\n- Race condition detection (parallel automation triggers)\n- Data integrity validation (does the data stay consistent?)\n\n**Infrastructure needed:**\n- Docker or isolated data directories for parallel environments\n- Headless runner already exists — just needs orchestration layer\n- Cost tracking per test run (Helicone sessions)\n- Test result aggregation and reporting\n- Rollback mechanism if autonomous mode breaks things\n\n**This is chaos engineering meets AI agents.** Netflix's Chaos Monkey but the monkey is an AI that invents new ways to break your system.",
      "category": "architecture",
      "status": "dismissed",
      "priority": "critical",
      "source": "chat 2026-02-08",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [
        "Discovers edge cases humans would never think of",
        "Continuous testing without writing test scripts",
        "Scales — can run 100 parallel test environments",
        "Self-improving — AI learns what breaks and tests those areas more",
        "Autonomous mode enables true self-healing systems",
        "Perfect for dogfooding validation before real use",
        "DevTrack testing itself is the ultimate vindaloop"
      ],
      "cons": [
        "AI cost could be high if running continuously",
        "Autonomous mode is risky — AI could break production",
        "Need robust isolation so test environments don't affect real data",
        "AI-generated test scenarios might be nonsensical or redundant",
        "Requires Docker or complex environment isolation",
        "Reporting/aggregation of findings is non-trivial"
      ],
      "open_questions": [
        "Docker containers vs separate data directories for isolation?",
        "How does the AI decide what scenarios to test?",
        "Should it learn from previous runs (vector DB of known failure modes)?",
        "How to prevent infinite loops or recursive chaos?",
        "What's the cost budget for a full test run?",
        "Can we use cheap models (GPT-4o-mini) for test generation and premium for analysis?",
        "How to aggregate findings from 100 parallel runs?",
        "Should this integrate with CI/CD or be on-demand only?",
        "Autonomous mode: how much can the AI change before requiring human review?"
      ],
      "notes": "DUPLICATE of IDEA-049. Created by DevTrack AI conversation. Dismissed as duplicate.",
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-049",
      "title": "AI-powered autonomous chaos testing — self-directed stress testing with virtual environments",
      "description": "An automated testing system where AI runs DevTrack headlessly in isolated virtual environments and invents its own test scenarios. The AI acts like a user: creates projects, adds backlog items, triggers automations, simulates file changes, tests edge cases, and reports back. Not traditional integration tests with fixed scripts — the AI **devises new situations** based on use cases and tries to break things.\n\n**How it works:**\n1. User sets constraints: max cost, max iterations, max time, number of parallel environments\n2. AI spins up N virtual DevTrack instances (Docker containers or separate data directories)\n3. AI generates test scenarios: \"What if a user creates 500 backlog items at once?\", \"What if automations trigger recursively?\", \"What if the user switches projects mid-conversation?\"\n4. AI executes scenarios via the headless runner and API calls\n5. AI observes: errors logged, performance metrics, data corruption, race conditions\n6. AI reports findings: issues discovered, edge cases hit, performance bottlenecks\n\n**Three modes:**\n- **Observer mode**: AI reports findings but doesn't edit code (safest)\n- **Assisted mode**: AI proposes fixes, user reviews before applying\n- **Autonomous mode**: AI fixes bugs it finds and commits (full self-healing)\n\n**Use cases:**\n- Stress testing before dogfooding on real projects\n- Continuous chaos testing in CI/CD\n- Discovering edge cases humans wouldn't think of\n- Load testing (100 concurrent chat sessions)\n- Race condition detection (parallel automation triggers)\n- Data integrity validation (does the data stay consistent?)\n\n**Infrastructure needed:**\n- Docker or isolated data directories for parallel environments\n- Headless runner already exists — just needs orchestration layer\n- Cost tracking per test run (Helicone sessions)\n- Test result aggregation and reporting\n- Rollback mechanism if autonomous mode breaks things\n\n**This is chaos engineering meets AI agents.** Netflix's Chaos Monkey but the monkey is an AI that invents new ways to break your system.",
      "category": "architecture",
      "status": "dismissed",
      "priority": "critical",
      "source": "chat 2026-02-08",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [
        "Discovers edge cases humans would never think of",
        "Continuous testing without writing test scripts",
        "Scales — can run 100 parallel test environments",
        "Self-improving — AI learns what breaks and tests those areas more",
        "Autonomous mode enables true self-healing systems",
        "Perfect for dogfooding validation before real use",
        "DevTrack testing itself is the ultimate vindaloop"
      ],
      "cons": [
        "AI cost could be high if running continuously",
        "Autonomous mode is risky — AI could break production",
        "Need robust isolation so test environments don't affect real data",
        "AI-generated test scenarios might be nonsensical or redundant",
        "Requires Docker or complex environment isolation",
        "Reporting/aggregation of findings is non-trivial"
      ],
      "open_questions": [
        "Docker containers vs separate data directories for isolation?",
        "How does the AI decide what scenarios to test?",
        "Should it learn from previous runs (vector DB of known failure modes)?",
        "How to prevent infinite loops or recursive chaos?",
        "What's the cost budget for a full test run?",
        "Can we use cheap models (GPT-4o-mini) for test generation and premium for analysis?",
        "How to aggregate findings from 100 parallel runs?",
        "Should this integrate with CI/CD or be on-demand only?",
        "Autonomous mode: how much can the AI change before requiring human review?"
      ],
      "notes": "DUPLICATE of IDEA-048. Created by DevTrack AI conversation. Dismissed as duplicate.",
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-050",
      "title": "Model delegation pattern — senior models trigger junior models for grunt work",
      "status": "captured",
      "category": "architecture",
      "priority": "critical",
      "description": "Senior AI models (Opus, GPT-5.2) should be able to delegate grunt work to junior models (Haiku, GPT-4o-mini). Pattern: when a weekly/nightly audit runs on a premium model, it triggers a junior model to first run over all commits, diffs, and changes to create a consolidated summary. The senior model then reads that summary instead of processing raw data itself. Junior model could run over 20+ commits, the entire DB, all recent changes — still cheaper than Opus doing a couple of diff operations. The senior model focuses on judgment and decision-making, not data gathering. Example flow: (1) Opus triggers Haiku to 'summarize all changes since last audit', (2) Haiku runs over commits, changelog, activity feed, outputs structured summary, (3) Opus reads summary + makes decisions about promotions, demotions, health ratings, issues. This is essentially a map-reduce pattern for AI operations.",
      "pros": [
        "Massive cost reduction — junior models handle 80% of the work",
        "Senior models focus on judgment, not data gathering",
        "Junior model can process entire codebases cheaply",
        "Natural separation of concerns in AI operations"
      ],
      "cons": [
        "Junior model summaries may miss nuance",
        "Adds latency (two-step process)",
        "Need to define the handoff format between models"
      ],
      "open_questions": [
        "What's the handoff format — structured JSON? Natural language summary?",
        "Should the senior model be able to ask for more detail on specific areas?",
        "How to handle when junior model makes a mistake in summarization?"
      ],
      "notes": "User insight: 'smaller models educating bigger ones quickly so the big models can defer down to the runs the smaller models already did.' GPT-4.0 was recently deprecated. Haiku can run over the whole database and 20 commits for less than Opus doing a couple of diff operations. Future group: Tiered Audit System (with IDEA-051, IDEA-055).",
      "source": "conversation session-8",
      "related_ideas": [
        "IDEA-051",
        "IDEA-014"
      ],
      "promoted_to": null,
      "tags": [
        "ai",
        "cost",
        "architecture"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-051",
      "title": "AI memory and context caching — per-automation memory, decision cache, semantic dedup",
      "status": "captured",
      "category": "architecture",
      "priority": "critical",
      "description": "Every automation run currently starts cold — no memory of what it decided before. This causes duplicates (3 identical weekly reports) and re-processing of unchanged data. Three-layer solution: (1) Per-automation memory — each automation gets data/automations/{id}/memory.json. AI writes its own notes at end of each run: what it found, what it changed, what to check next time. Next run loads this as part of system prompt. (2) Decision cache — data/ai/decisions-cache.json. Short-lived cache of recent AI decisions: 'I triaged IDEA-034 as exploring at 2am, don't re-triage for 7 days.' Prevents AI from churning on same items every run. (3) Semantic dedup — before creating brain notes, changelog entries, etc, the automation prompt includes recent entries of that type so AI can check 'did I already write this?' This is cheap to build and eliminates the duplicate/churn problem without needing a vector DB.",
      "pros": [
        "Eliminates duplicate brain notes and changelog entries",
        "AI remembers its previous decisions",
        "Prevents re-processing unchanged data",
        "Cheap to build — just JSON files as memory"
      ],
      "cons": [
        "Memory files grow over time (need their own archival)",
        "AI might over-rely on stale memory",
        "Decision cache needs TTL management"
      ],
      "open_questions": [
        "How much memory context can we inject without blowing up token costs?",
        "Should memory be append-only or can the AI rewrite its own notes?",
        "TTL on decision cache — 7 days? Configurable per decision type?"
      ],
      "notes": "User insight: 'we probably need some kind of way to cache memory for the AI... quick context retrieval of what it has decided in the past.' This is the structural fix for automation garbage data. Without this, turning automations on will create duplicates. Future group: Tiered Audit System (with IDEA-050, IDEA-055).",
      "source": "conversation session-8",
      "related_ideas": [
        "IDEA-050",
        "IDEA-014",
        "IDEA-047"
      ],
      "promoted_to": null,
      "tags": [
        "ai",
        "memory",
        "architecture",
        "automations"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-052",
      "title": "Entity grouping and parenting — ideas with sub-ideas, roadmap hierarchy, epics in UI",
      "status": "promoted",
      "category": "ux",
      "priority": "high",
      "description": "Multiple entity types need parent/child relationships: (1) Ideas need sub-ideas — a big idea like 'AI-unique views' has multiple child ideas under it. (2) Roadmap items need grouping — epics exist in the data model but aren't displayed in the UI anywhere. (3) Design docs, RFDs, ADRs — these doc types exist conceptually but aren't surfaced distinctly. The user can't see the hierarchy that exists in the data. Epics have item_count and completed_count but no UI displays them. Milestones have version numbers but no UI shows version tracking. Other tools (Linear, Notion) typically use one level: Epic→Issues, Project→Tasks.",
      "pros": [
        "Epics become visible in the UI",
        "Ideas can be organized under themes",
        "Roadmap hierarchy matches how people think about work",
        "Data model already supports it (epic_id, milestone_id fields exist)"
      ],
      "cons": [
        "Deep hierarchy gets confusing quickly",
        "Need UI for creating/managing parent-child relationships",
        "Navigation between levels needs to be intuitive"
      ],
      "open_questions": [
        "One level of grouping (parent→children) or full tree?",
        "How do other tools handle this? Linear uses Projects→Issues, Notion uses nested pages",
        "Should ideas have a 'theme' tag or actual parent-child?",
        "How to display hierarchy in kanban view without clutter?"
      ],
      "notes": "PROMOTED TO ROADMAP (epic-hierarchy-ui). SHIPPED in session 9: Roadmap By Epic view (items grouped under epics with progress bars, colored dots, inline epic editor), Ideas By Group view (connected-component clustering via related_ideas), cross-entity linking (open issues per epic). Core epic visibility DONE. Remaining (Ideas parent_idea field, deeper nesting) was out of original scope — user need fully met.",
      "source": "conversation session-8",
      "related_ideas": [
        "IDEA-034",
        "IDEA-062",
        "IDEA-063",
        "IDEA-064"
      ],
      "promoted_to": null,
      "tags": [
        "ui",
        "data-model",
        "hierarchy"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-09"
    },
    {
      "id": "IDEA-053",
      "title": "Smart archival — content-aware, not timeline-based",
      "status": "captured",
      "category": "architecture",
      "priority": "high",
      "description": "The current archival-system backlog item proposes rolling windows (archive after 30 days). User pushback: timeline-based archival will lose active items. Changelogs and session logs shouldn't be archived by date — they're history. Better approach: (1) Archive by status, not time — resolved issues can be archived, open ones never. Completed roadmap items in 'shipped' can be summarized. (2) Summarize, don't delete — monthly summaries of changelog, session velocity, issues resolved. Raw data moves to archive/ but summaries stay. (3) Never archive: brain notes (unless dismissed), active ideas, open issues, in-progress roadmap items. (4) Consider: what actually needs archival? Maybe just conversations (large), activity feed (grows fast), and codebase analysis snapshots.",
      "pros": [
        "Won't accidentally lose active items",
        "Summaries preserve context without bulk",
        "Status-based archival is more natural"
      ],
      "cons": [
        "More complex than simple time-based archival",
        "Need to define 'what's archivable' per entity type",
        "Summaries lose detail"
      ],
      "open_questions": [
        "What actually gets large enough to need archival?",
        "Is the problem even real yet or premature optimization?",
        "Should AI decide what to archive based on context?"
      ],
      "notes": "User: 'Timeline automated archival... if things are active we're gonna lose them. You don't want to archive off changelogs and session logs.' Legitimate concern — a time-based rolling window would archive entries still referenced by open issues.",
      "source": "conversation session-8",
      "related_ideas": [
        "IDEA-042",
        "IDEA-058"
      ],
      "promoted_to": null,
      "tags": [
        "data",
        "archival",
        "architecture"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-054",
      "title": "Session heartbeat + stale session detector — structural fix for orphaned sessions",
      "description": "Sessions 6 & 7 were orphaned because the external AI forgot to call POST /session/end. Behavioral compliance is unreliable. Need a structural mechanism: (1) Session heartbeat — bump last_active timestamp on every activity event (zero-cost, activity system already fires). (2) Stale session automation — scheduled check every 30-60min, finds sessions with status=active but last_active older than configurable threshold (e.g. 2h). Marks session 'stale', fires session_stale trigger. (3) DevTrack AI circle-back — on session_stale trigger, AI reviews session activity, writes retro from available data, closes session gracefully. This is the safety net for when the external AI drifts. (4) Multi-user aware — sessions are per-user and can overlap. Stale detector checks each independently. No assumption that new session = old one ended.",
      "category": "architecture",
      "status": "promoted",
      "priority": "high",
      "source": "conversation session-8",
      "pros": [
        "Structurally prevents orphaned sessions — no behavioral dependency",
        "Zero-cost heartbeat — piggybacks on existing activity system",
        "Multi-user safe — parallel sessions work correctly",
        "AI circle-back means sessions always get proper retros and velocity data",
        "Configurable threshold — tight for solo dev, loose for teams"
      ],
      "cons": [
        "Stale != done — AI retro from incomplete data may miss context",
        "Need to distinguish 'thinking break' from 'actually done'",
        "Adds another automation to the scheduler"
      ],
      "open_questions": [
        "What's the right staleness threshold? 2 hours? Configurable per user?",
        "Should stale sessions auto-close or just notify and wait for human confirmation?",
        "Should the heartbeat be implicit (any activity = heartbeat) or explicit (periodic ping)?",
        "How does this interact with the session_ended trigger for session-end-audit?"
      ],
      "tags": [
        "sessions",
        "reliability",
        "automation",
        "multi-user"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-09",
      "notes": "PROMOTED TO ROADMAP (session-heartbeat-stale-detector). Direct structural fix for ISS-020 root cause. Sessions 6-7 orphaned because external AI forgot to end them. Behavioral compliance unreliable. Solution: (1) Session heartbeat (bump last_active on every activity event), (2) Stale session detector (scheduled check for inactive sessions), (3) DevTrack AI circle-back (on session_stale trigger, write retro, close gracefully), (4) Multi-user aware. Zero-cost heartbeat, configurable threshold, structural safety net.",
      "related_ideas": [
        "IDEA-024",
        "IDEA-025"
      ]
    },
    {
      "id": "IDEA-055",
      "promoted_to": "tiered-audit-system",
      "title": "Tiered audit system — file change (pennies), session (dimes), weekly (dollars)",
      "description": "Current session-end audit costs $2.82 on Sonnet for 25 tool calls over 164s — too expensive for per-session use. On a large codebase (Pillar = 150K lines vs DevTrack = 30K) this balloons to $10+. Need a tiered audit architecture:\n\n(1) FILE CHANGE AUDIT (pennies): On any file change, run a micro-audit. Use cached AI state so it doesn't need to re-read the whole system. Just: what changed? does the cache need updating? does a changelog entry make sense? Should take <5 tool calls, cost <$0.01. Can run continuously, not gated to 60s intervals.\n\n(2) SESSION AUDIT (dimes): On session close, diff commits from session start to session end. Only audit what's relevant to the session — don't re-read the entire system. AI already has cached state from file-change audits. Cost target: $0.10-0.30.\n\n(3) NIGHTLY AUDIT (low dollars): Broader sweep but still scoped. Check for stale data, contradictions, things the external AI forgot. Has benefit of all the cached state from the day's file-change audits. Cost target: $0.50-1.00.\n\n(4) WEEKLY AUDIT (dollars): Full system audit. This is where the $2-3 spend makes sense. Deep review, cross-entity consistency, strategic recommendations. Cost target: $2-5.\n\nKey enabler: AI state cache that gets incrementally updated on every file change, so larger audits don't start cold.",
      "category": "architecture",
      "status": "validated",
      "priority": "critical",
      "source": "conversation session-8 (post-close)",
      "pros": [
        "Reduces per-session cost from $2.82 to ~$0.10-0.30",
        "File change audits can run continuously at pennies",
        "Scales to large codebases — cost tied to change volume, not codebase size",
        "Cached state means no audit starts cold",
        "Each tier builds on the work of the tier below it"
      ],
      "cons": [
        "Cache staleness — if cache drifts from reality, all tiers are wrong",
        "More complex architecture than current 'run everything every time'",
        "Need to define what's 'relevant to the session' programmatically"
      ],
      "open_questions": [
        "What format is the AI cache? JSON summary? Embedding? Plain text briefing?",
        "How does the file-change micro-audit know what's worth caching vs noise?",
        "Can we use git diff as the primary input for session audits instead of reading all files?",
        "What's the cost floor — can we get file-change audits under $0.005 with Haiku?"
      ],
      "tags": [
        "cost",
        "architecture",
        "automation",
        "scalability"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-09",
      "notes": "PROMOTED TO ROADMAP. Current session-end audit costs $2.82 (25 tool calls, 164s). On large codebase (Pillar 150K lines) would balloon to $10+. Tiered architecture: (1) File change micro-audit (Haiku, <$0.01, continuous), (2) Session audit ($0.10-0.30 using cached state + git diff), (3) Nightly audit ($0.50-1.00), (4) Weekly full audit ($2-5). Key enabler: AI state cache updated incrementally. This is THE cost-effective path to continuous change tracking. XL item — should be broken down into phases.",
      "related_ideas": [
        "IDEA-057",
        "IDEA-059",
        "IDEA-014"
      ]
    },
    {
      "id": "IDEA-056",
      "promoted_to": "auto-commit-session-close",
      "title": "Auto-commit on session close — detect uncommitted work, commit to correct branch",
      "description": "When a session ends, the system should: (1) Check if there are uncommitted changes. (2) If yes, check if auto-commit is enabled in settings. (3) Detect the current branch (main, feature, etc.) and commit there. (4) Use the session's changelog entries and activity to generate a meaningful commit message. (5) Optionally push if configured. This should be a setting in the Settings UI: 'Auto-commit on session close' with options: off, commit only, commit and push. The external AI (Cursor) should also be doing this, but the structural safety net is DevTrack handling it if they don't — same pattern as the session heartbeat.",
      "category": "feature",
      "status": "promoted",
      "priority": "high",
      "source": "conversation session-8 (post-close)",
      "pros": [
        "Prevents the 27-uncommitted-files situation we're in right now",
        "Structural safety net — doesn't depend on external AI remembering",
        "Auto-generated commit messages from session data are probably better than manual ones",
        "Setting gives user control over behavior"
      ],
      "cons": [
        "Auto-committing to main could be dangerous — need branch awareness",
        "Partial/broken work shouldn't be auto-committed — needs a 'build passes' gate?",
        "Git operations from the server process could conflict with user's IDE git"
      ],
      "open_questions": [
        "Should it create a branch if the user is on main? Or commit directly?",
        "How to handle merge conflicts if the user and DevTrack both write?",
        "Should there be a 'review before commit' step or fully automatic?",
        "What about .gitignore — data files may not be in git"
      ],
      "tags": [
        "git",
        "sessions",
        "automation",
        "settings"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-09",
      "notes": "PROMOTED TO ROADMAP (auto-commit-session-close). Session-end audit found 27 uncommitted files. On session close: detect uncommitted changes, check auto-commit setting, detect branch, generate commit message from session activity, commit (optionally push). Setting in Settings UI: off/commit/commit+push. Structural safety net — doesn't depend on external AI remembering. Quick M-sized win with high impact."
    },
    {
      "id": "IDEA-057",
      "title": "Stale data watchdog — track expected updates, auto-fix if external AI forgets",
      "description": "When DevTrack detects that something should have been updated (e.g., an idea was discussed, a feature was shipped, a session ended), it starts a watchdog timer. If the external AI doesn't update the relevant entities within X minutes, DevTrack's AI does it. Example: user discusses updating IDEA-034 in chat → DevTrack notes 'IDEA-034 should be updated' → waits 10 minutes → checks if IDEA-034 was modified → if not, DevTrack AI updates it based on the conversation context. This prevents stale data without requiring massive periodic audits — it's event-driven staleness detection.",
      "category": "architecture",
      "status": "captured",
      "priority": "medium",
      "source": "conversation session-8 (post-close)",
      "pros": [
        "Prevents stale data without expensive periodic audits",
        "Event-driven — only fires when something is expected to change",
        "Structural enforcement of data freshness"
      ],
      "cons": [
        "Needs to understand 'what should have been updated' — requires conversation analysis",
        "Could create noise if it acts on false positives",
        "Overlaps with the tiered audit system"
      ],
      "open_questions": [
        "How does it know what 'should' have been updated?",
        "What's the right timeout before intervention?",
        "Should it notify the user before auto-fixing or just do it?"
      ],
      "tags": [
        "automation",
        "data-quality",
        "architecture"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "related_ideas": [
        "IDEA-055",
        "IDEA-059"
      ]
    },
    {
      "id": "IDEA-058",
      "title": "Codebase scope controls — exclude node_modules, vendor, third-party from all AI scans",
      "description": "All AI audits, scanners, and file watchers must respect scope boundaries. Never scan node_modules, vendor directories, .git, build output, or any third-party code. Only scan: (1) User's source code. (2) DevTrack data directory. (3) Config files at project root. This is critical for cost control on large codebases — Pillar at 150K lines would be insanely expensive if the AI is also reading 500K+ lines of dependencies. Should be configurable in settings: include/exclude glob patterns.",
      "category": "architecture",
      "status": "captured",
      "priority": "high",
      "source": "conversation session-8 (post-close)",
      "pros": [
        "Massive cost reduction on real-world projects",
        "Faster scans — less data to process",
        "More relevant results — no noise from third-party code"
      ],
      "cons": [
        "Some projects have important vendor code that should be scanned",
        "Need sensible defaults that work across project types"
      ],
      "open_questions": [
        "Default excludes: node_modules, .git, dist, build, vendor — what else?",
        "Should we read .gitignore and use that as the exclude base?",
        "How to handle monorepos with multiple packages?"
      ],
      "tags": [
        "cost",
        "scalability",
        "settings"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "related_ideas": [
        "IDEA-053",
        "IDEA-055"
      ]
    },
    {
      "id": "IDEA-059",
      "title": "Changelog auto-management — DevTrack AI writes changelogs, not external AI",
      "description": "Stop asking the external AI (Cursor) to write changelog entries. It's unreliable (context drift) and creates friction. Instead: DevTrack AI should auto-generate changelog entries from git diffs, activity events, and session data. The file-change micro-audit (IDEA-055 tier 1) already sees what changed — it should write the changelog entry. The session audit (tier 2) rolls up the session's changes into a session summary. The external AI should focus on code — DevTrack handles all the project management bookkeeping. This is the separation of concerns principle from IDEA-014 applied to changelogs specifically.",
      "category": "process",
      "status": "captured",
      "priority": "medium",
      "source": "conversation session-8 (post-close)",
      "pros": [
        "Removes burden from external AI — less context drift",
        "Changelogs are always accurate because they're generated from actual changes",
        "Separation of concerns: coding AI codes, DevTrack AI tracks"
      ],
      "cons": [
        "Auto-generated changelogs may miss intent/context that the coder knows",
        "Need to balance auto-generation with manual override capability"
      ],
      "open_questions": [
        "Should auto-changelogs be 'draft' status until confirmed?",
        "How granular — one entry per commit? per feature? per session?",
        "How does the user annotate/override auto-generated entries?"
      ],
      "tags": [
        "automation",
        "changelog",
        "process"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08",
      "related_ideas": [
        "IDEA-055",
        "IDEA-057",
        "IDEA-014"
      ]
    },
    {
      "id": "IDEA-060",
      "promoted_to": "audits-tab",
      "title": "Audits tab — view automation results, thinking chain, diffs, costs, suggestions",
      "description": "When an automation runs (session audit, nightly audit, weekly report, etc.), the results are invisible. The activity log shows 'took 164s' but not: what it changed, what it thought, what it decided not to act on, how much it cost, what files it touched. Need an Audits tab (or section within Activity) where each automation run has an expandable result card showing:\n\n(1) SUMMARY — plain English: 'Updated context recovery, found ISS-034, verified 6 open issues, flagged 27 uncommitted files.'\n(2) CHANGES MADE — list of files modified with before/after diffs or descriptions.\n(3) SUGGESTIONS NOT ACTED ON — things the AI noticed but didn't change (e.g., 'ai-watcher has been in_progress for 9 days — consider updating status').\n(4) THINKING CHAIN — condensed reasoning: why it made each decision.\n(5) COST — tokens used, model, estimated cost.\n(6) TOOL CALLS — expandable list of all tools called with args and results.\n\nThis requires the headless runner to return structured results that get persisted, not just logged to console.",
      "category": "feature",
      "status": "promoted",
      "priority": "critical",
      "source": "conversation session-8 (post-close)",
      "pros": [
        "Full transparency into what DevTrack AI is doing autonomously",
        "Enables trust — user can verify AI decisions",
        "Cost visibility per automation run",
        "Suggestions-not-acted-on creates a natural approval workflow"
      ],
      "cons": [
        "Storing full audit results per run could get large",
        "Thinking chain may be verbose — needs summarization"
      ],
      "open_questions": [
        "Store results in data/automations/{id}/runs/*.json?",
        "How many runs to keep? Rolling window?",
        "Should the AI explicitly separate 'changes made' from 'suggestions' in its output?"
      ],
      "tags": [
        "ui",
        "transparency",
        "automation",
        "trust"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-09",
      "related_ideas": [
        "IDEA-043",
        "IDEA-044",
        "IDEA-061",
        "IDEA-046"
      ],
      "notes": "PROMOTED TO ROADMAP (audits-tab). Automation results invisible. Need Audits tab showing: summary, changes made, suggestions not acted on, thinking chain, cost, tool calls. Enables trust, transparency, cost visibility, suggestion-with-approval workflow. SHIPPED in session 9: full audit recording pipeline + Audits UI with stats, filters, run cards, expandable detail. Critical for autonomous AI operations."
    },
    {
      "id": "IDEA-061",
      "promoted_to": "autonomy-levels",
      "title": "Autonomy levels — fully autonomous vs suggestion-with-approval mode",
      "description": "Not all users want DevTrack AI to make changes autonomously. Enterprise users may need approval workflows. Add an autonomy setting with levels:\n\n(1) FULL AUTONOMY — DevTrack AI makes all changes directly. Ideal for solo devs / power users who trust the system.\n(2) SUGGEST & APPROVE — DevTrack AI identifies changes but presents them as 1-click approval cards in the Audits tab. 'I want to promote IDEA-034 to roadmap' → user clicks Approve or Dismiss. Batch approval for multiple suggestions.\n(3) READ-ONLY — DevTrack AI only observes and reports. No changes made. Useful for evaluation / onboarding.\n\nThis is a per-user or per-project setting in Settings. The automation prompt should be aware of the autonomy level so the AI knows whether to execute changes or just recommend them.",
      "category": "feature",
      "status": "promoted",
      "priority": "high",
      "source": "conversation session-8 (post-close)",
      "pros": [
        "Opens DevTrack to enterprise users who need approval workflows",
        "Builds trust during onboarding — start read-only, graduate to autonomous",
        "1-click approval is great UX — all the AI intelligence, user stays in control",
        "Suggestions queue becomes a natural todo list"
      ],
      "cons": [
        "Suggestion mode creates a queue that could pile up if user doesn't act",
        "Need to handle stale suggestions (approved after context changed)",
        "More complex prompt engineering — AI needs to know its autonomy level"
      ],
      "open_questions": [
        "Can autonomy vary per entity type? (e.g., autonomous for ideas, approval for roadmap)",
        "How long do suggestions stay valid before auto-expiring?",
        "Should there be a 'batch approve all' option for busy users?"
      ],
      "tags": [
        "settings",
        "enterprise",
        "autonomy",
        "trust"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-09",
      "related_ideas": [
        "IDEA-060",
        "IDEA-043"
      ],
      "notes": "PROMOTED TO ROADMAP (autonomy-levels). Three modes: (1) Full autonomy (AI makes all changes), (2) Suggest & approve (1-click approval cards in Audits tab), (3) Read-only (observe and report). Per-project setting. Automation prompts adapt to level. Opens DevTrack to enterprise users. Depends on audits-tab for suggestion rendering."
    },
    {
      "id": "IDEA-062",
      "title": "Analytics dashboard — velocity charts, issue burn rate, health trends",
      "description": "Build a dedicated analytics/metrics view with real charts. All data already exists in velocity.json (per-session: items shipped, points, issues found/resolved). Needs: (1) velocity line chart (items + points over sessions), (2) issue burn rate (discovered vs resolved per session), (3) system health trend over time, (4) session throughput histogram. Reference Linear's cycle analytics and burndown charts for design inspiration. Dark theme, thin lines, minimal axes.",
      "category": "feature",
      "status": "captured",
      "priority": "high",
      "source": "session-9 discussion",
      "related_ideas": [
        "IDEA-063",
        "IDEA-064",
        "IDEA-013"
      ],
      "promoted_to": null,
      "pros": [
        "All data already collected — zero backend work needed",
        "Highest-impact visual improvement possible",
        "Makes velocity tracking tangible instead of JSON numbers",
        "Linear-inspired cycle charts as proven design reference"
      ],
      "cons": [
        "Needs a charting library (recharts, chart.js, or visx)",
        "Design polish required to match Linear quality"
      ],
      "open_questions": [
        "Which charting library? recharts is lightest, visx is most flexible",
        "Should charts be on a dedicated Analytics tab or embedded in Dashboard?",
        "What time granularity? Per-session vs per-day vs per-week?"
      ],
      "notes": "Linear reference: their cycle burndown charts, debugging stats, and velocity graphing are best-in-class. Dark theme with thin colored lines on dark bg. Session-based (not sprint-based) since dev-track sessions are our equivalent of cycles. Data source: data/metrics/velocity.json already has per-session breakdowns.",
      "tags": [
        "analytics",
        "charts",
        "linear-inspired",
        "high-impact"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-063",
      "title": "Timeline / roadmap visualization — horizontal bar chart of work over time",
      "description": "Build a horizontal timeline view for roadmap items, showing items as bars spanning their start-to-completion dates. Like Linear's timeline view or a lightweight Gantt chart. Needs target_date field on RoadmapItem (currently only has started/completed). Show epics as grouped swim lanes. Color by status or horizon.",
      "category": "feature",
      "status": "captured",
      "priority": "medium",
      "source": "session-9 discussion",
      "related_ideas": [
        "IDEA-062",
        "IDEA-064"
      ],
      "promoted_to": null,
      "pros": [
        "Visual representation of 'what ships when'",
        "Natural grouping by epic on the Y-axis",
        "Linear's timeline view is very popular — proven UX"
      ],
      "cons": [
        "Needs target_date field added to RoadmapItem type and UI",
        "Horizontal scrolling UX is tricky to get right",
        "Less useful for solo dev than for teams"
      ],
      "open_questions": [
        "Is target_date per-item or per-epic sufficient?",
        "How to handle items with no dates? Omit from timeline?",
        "Should this be a separate tab or a view mode within Roadmap?"
      ],
      "notes": "Linear reference: their timeline view shows items as horizontal bars grouped by project, with drag-to-resize for dates. Beautiful design with clean axis labels. For solo dev, this is lower priority than analytics but still valuable for planning.",
      "tags": [
        "timeline",
        "visualization",
        "linear-inspired"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-064",
      "title": "Rich filtering and custom views across all entities",
      "description": "Add a filter bar or command-K style filtering to any view. Filter by status, priority, epic, tags, assignee, date range. Save filter presets as 'custom views.' Like Linear's filter pills or Notion's database views. Should work across Roadmap, Issues, Ideas, Changelog.",
      "category": "feature",
      "status": "captured",
      "priority": "medium",
      "source": "session-9 discussion",
      "related_ideas": [
        "IDEA-062",
        "IDEA-063"
      ],
      "promoted_to": null,
      "pros": [
        "Makes large item lists manageable",
        "Saved views reduce repetitive filtering",
        "Linear and Notion prove this UX is essential at scale"
      ],
      "cons": [
        "Significant UI engineering — filter bar, save/load, URL params",
        "Cross-entity filtering adds query complexity"
      ],
      "open_questions": [
        "Filter bar inline or command-palette style?",
        "Save filters to localStorage or to data file?",
        "Which views need filtering first? Roadmap and Issues most likely"
      ],
      "notes": "Linear reference: pill-style filter chips that stack horizontally. Click to add filter, dropdown to pick field + value. Very clean. Lower priority than analytics and epic views since we don't have enough items yet to need filtering.",
      "tags": [
        "filtering",
        "views",
        "linear-inspired"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-065",
      "title": "Chat agent as investigation + handoff layer — rich entities as artifacts",
      "description": "The chat agent's superpower isn't code editing — it's deep investigation followed by rich documentation. Pattern: user reports bug → agent reads files, traces root cause → creates ISS-XXX with root cause, affected files, code snippets, proposed solutions → user tells Cursor 'fix ISS-XXX'. The issue becomes a handoff artifact that gives the coding AI all context. This means entity detail richness matters more than we've been treating it. Issues should have full investigation notes, ideas should have thorough pros/cons/questions, roadmap items should have acceptance criteria. The agent should be trained to maximize entity quality.",
      "category": "architecture",
      "status": "exploring",
      "priority": "high",
      "source": "session-9 insight",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [
        "Leverages agent's strength (reading, reasoning) without its weakness (writing code)",
        "Creates persistent artifacts that survive conversation context limits",
        "Bridges the gap between DevTrack and external coding AI (ISS-012)",
        "Proven in session 9 — ISS-035 found and documented by chat agent, fixed by Cursor agent"
      ],
      "cons": [
        "Requires discipline to keep entity detail rich — AI might create thin entities",
        "Handoff UX needs polish — how does user 'send' an issue to Cursor?"
      ],
      "open_questions": [
        "Should issues have a 'handoff_instructions' field specifically for external AI?",
        "Can we auto-generate a Cursor-ready prompt from an issue entity?",
        "How do we track that an issue was handed off and implemented externally?"
      ],
      "notes": "This pattern was validated live in session 9. The chat agent found ISS-035 (shallow merge bug), created the issue with full investigation detail, and the Cursor agent implemented the fix. This is the bridge pattern from ISS-012.",
      "tags": [
        "architecture",
        "ai-handoff",
        "investigation-pattern"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-066",
      "title": "Bi-directional context push — DevTrack pushes context to agent mid-conversation",
      "description": "Currently the chat agent pulls context from DevTrack (reads files, queries entities). But there's no feedback loop where DevTrack pushes context back to the agent. Example: agent creates an issue, DevTrack's dedup check finds it overlaps with ISS-012 — but that context never reaches the agent mid-conversation. Could inject system messages or tool results that say 'Note: this issue overlaps with ISS-012 (high priority, open since session 3)'. Also: when agent reads a roadmap item, auto-inject related issues/ideas as context.",
      "category": "feature",
      "status": "captured",
      "priority": "medium",
      "source": "session-9 upfactoring",
      "related_ideas": [
        "IDEA-065"
      ],
      "promoted_to": null,
      "pros": [
        "Agent makes better decisions with more context",
        "Prevents duplicate work — agent knows what already exists",
        "Feels more like a real teammate that knows the project"
      ],
      "cons": [
        "Adds complexity to tool execution layer",
        "Could bloat context window with too much injected info",
        "Hard to decide what's relevant to inject vs noise"
      ],
      "open_questions": [
        "Inject as system messages or as tool result annotations?",
        "How aggressive should the push be? Every tool call or only on key operations?",
        "Can we use embeddings to determine relevance of pushed context?"
      ],
      "notes": "",
      "tags": [
        "ai-intelligence",
        "context-management"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-067",
      "title": "View-level data lifecycle — each view manages its own freshness instead of global refresh",
      "description": "The refreshKey pattern (ISS-039) was a symptom of not having proper data lifecycle per view. Each view should decide: (1) fetch on mount, (2) poll on interval if it needs live data (Dashboard every 30s, Activity every 10s), (3) subscribe to specific WS event types it cares about (Issues view listens for issue_created/resolved, not every file change), (4) never remount just because unrelated data changed. Could implement a lightweight useEntityData hook that handles mount-fetch + WS-subscription + optimistic updates.",
      "category": "infrastructure",
      "status": "captured",
      "priority": "high",
      "source": "session-9 ISS-039 fix",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [
        "Eliminates all the state-loss bugs from global refresh",
        "Views only re-render when their data actually changes",
        "Foundation for optimistic updates and real-time collaboration"
      ],
      "cons": [
        "Requires touching every view component",
        "Need to define which WS events each view cares about",
        "Polling adds slight server load"
      ],
      "open_questions": [
        "Start with a useEntityData hook or just add useEffect polling per view?",
        "Which views need live data? Dashboard, Activity, Audits. Others probably don't.",
        "Should we add typed WS event subscriptions or just filter client-side?"
      ],
      "notes": "Current state after ISS-039 fix: views fetch on mount but never refresh. This is better than the remount-everything pattern but means data goes stale. Medium priority since most views are fine with mount-only fetch for now.",
      "tags": [
        "infrastructure",
        "state-management",
        "real-time"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-068",
      "title": "Auto-generate Cursor-ready prompts from DevTrack entities",
      "description": "When the chat agent creates a rich issue (ISS-035 with root cause, files, proposed fix), generate a one-click 'Copy for Cursor' prompt that includes: issue title, root cause, affected files with line numbers, proposed solution approach, acceptance criteria. User clicks the button, pastes into Cursor, and the coding AI has full context without re-investigation. Could also work for roadmap items with acceptance criteria and for ideas being promoted to implementation.",
      "category": "feature",
      "status": "captured",
      "priority": "medium",
      "source": "session-9 upfactoring",
      "related_ideas": [
        "IDEA-065"
      ],
      "promoted_to": null,
      "pros": [
        "Eliminates the manual context transfer between DevTrack and coding AI",
        "Standardizes the handoff format",
        "Makes the investigation→implementation pipeline seamless"
      ],
      "cons": [
        "Prompt format depends on which coding AI the user uses",
        "Needs to be kept updated as entity schema evolves"
      ],
      "open_questions": [
        "Button on entity cards or a global 'Export to AI' action?",
        "Different prompt templates for different target AIs?",
        "Should it include the full entity or a condensed handoff summary?"
      ],
      "notes": "",
      "tags": [
        "ai-handoff",
        "developer-experience"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-069",
      "title": "AI State Cache — compressed project state summary for efficient audits",
      "description": "A data/ai/state-cache.json file containing a compressed summary of project state (~2-5K tokens instead of 130K). Systems list, roadmap status, open issues, recent changelog, velocity stats. Updated incrementally after every audit/agent run. All subsequent audits load the cache instead of re-reading every data file. Paired with change detection: diff current state vs cache to determine what actually changed before firing an expensive AI call.",
      "category": "architecture",
      "status": "implemented",
      "priority": "critical",
      "source": "session-10 (Helicone log audit — $36 docs generation showed every run rebuilds from scratch)",
      "related_ideas": [
        "IDEA-051",
        "IDEA-055"
      ],
      "promoted_to": null,
      "pros": [
        "90% context reduction per audit call",
        "Enables incremental audits",
        "Foundation for tiered audit system",
        "Reduces rate limit pressure"
      ],
      "cons": [
        "Cache can become stale if not updated",
        "Need to handle cache invalidation",
        "Adds a new data file to maintain"
      ],
      "open_questions": [
        "What's the right cache format?",
        "How often to rebuild cache from scratch?",
        "Should cache be per-automation or global?"
      ],
      "notes": "Real data: docs init used 2.1M input tokens across 17 requests. With state cache, most of that context could be pre-summarized.",
      "tags": [
        "ai",
        "cost",
        "architecture",
        "caching"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-070",
      "title": "Per-doc generation instead of monolithic agent run",
      "description": "Instead of one agent that generates all 14 docs in a single 30-iteration run (paying 130K tokens per iteration), run separate focused agents per doc. Each agent gets: the state cache + relevant source files for THAT doc only. Benefits: (1) if one fails, others succeed, (2) much smaller context per call, (3) naturally resilient to server restarts (completed docs stay), (4) can parallelize. Cost: probably $1-2 per doc on Sonnet instead of $2.50 per iteration on Opus.",
      "category": "architecture",
      "status": "implemented",
      "priority": "high",
      "source": "session-10 (agent wasted $14 fumbling with tool calls across iterations)",
      "related_ideas": [
        "IDEA-069"
      ],
      "promoted_to": null,
      "pros": [
        "Natural resilience — each doc is atomic",
        "Smaller context per call",
        "Parallelizable",
        "Can route different docs to different models"
      ],
      "cons": [
        "More API calls total",
        "Each doc agent lacks cross-doc context",
        "Need orchestration layer"
      ],
      "open_questions": [
        "Parallel or sequential?",
        "How to share context between doc agents?",
        "Does per-doc actually cost less than monolithic?"
      ],
      "notes": "",
      "tags": [
        "ai",
        "cost",
        "docs",
        "architecture"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-071",
      "title": "Model tier setting per automation and per feature — power users choose Opus, default Sonnet",
      "description": "Let users configure which model tier to use for each operation type. Docs initialization: Opus (default for power users) or Sonnet (default for cost-conscious). Doc updates: always Sonnet. Session audits: Sonnet. Nightly: Sonnet. Weekly: Opus. Per-automation override in the automation config. Per-feature override in ai/config.json. UI: dropdown per automation in Settings.",
      "category": "feature",
      "status": "exploring",
      "priority": "medium",
      "source": "session-10 (user question: is Opus worth $30 vs Sonnet $5 for initialization?)",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [
        "Users control their own cost/quality tradeoff",
        "Power users get premium quality",
        "Default to affordable"
      ],
      "cons": [
        "More config complexity",
        "Need to test quality across tiers"
      ],
      "open_questions": [
        "Should this be per-user or per-project?",
        "What's the default for new users?"
      ],
      "notes": "",
      "tags": [
        "settings",
        "ai",
        "cost"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-072",
      "title": "Wire docs generation into project initialization flow",
      "description": "POST /api/v1/init currently scans codebase and populates entities but doesn't generate docs. When a new project is initialized (like porting to Landmark), the init flow should: (1) scan codebase, (2) create systems/roadmap/issues, (3) generate full doc structure via the docs generation pipeline. User sees a complete wiki when they first open the Docs tab — no need to click Initialize separately.",
      "category": "feature",
      "status": "implemented",
      "priority": "high",
      "source": "session-10 (user: 'when we initialize in a new project, you need to run everything top to bottom, including generating the docs')",
      "related_ideas": [
        "IDEA-070"
      ],
      "promoted_to": null,
      "pros": [
        "Seamless first-time experience",
        "No manual steps",
        "Docs ready from day one"
      ],
      "cons": [
        "Init takes longer",
        "More expensive first-run cost"
      ],
      "open_questions": [
        "Run docs generation async or block until done?",
        "Show progress UI during init?"
      ],
      "notes": "",
      "tags": [
        "initialization",
        "docs",
        "ux"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    },
    {
      "id": "IDEA-073",
      "title": "Helicone log ingestion — pull request logs into DevTrack for cost analysis and audit review",
      "description": "Build a Helicone integration that pulls request logs via their API (clickhouse endpoint works, point query endpoint is slow). Store summarized request data in DevTrack. Show cost breakdowns per automation, per session, per feature. Enable the AI to audit its own API usage patterns and suggest optimizations. User already exports JSONL/CSV from Helicone — we should do this automatically.",
      "category": "feature",
      "status": "exploring",
      "priority": "medium",
      "source": "session-10 (user: 'can you audit the logs end to end in Helicone through the API')",
      "related_ideas": [],
      "promoted_to": null,
      "pros": [
        "Self-aware cost tracking",
        "AI can optimize its own usage",
        "Rich analytics in DevTrack"
      ],
      "cons": [
        "Another integration to maintain",
        "Helicone API can be slow",
        "Data duplication"
      ],
      "open_questions": [
        "Pull on schedule or on demand?",
        "How much request data to store?",
        "Privacy implications of storing full prompts?"
      ],
      "notes": "Clickhouse endpoint works for bulk queries. Point query endpoint returned empty. Need org ID header.",
      "tags": [
        "helicone",
        "cost",
        "analytics"
      ],
      "created": "2026-02-08",
      "updated": "2026-02-08"
    }
    ,
    {
      "id": "IDEA-074",
      "title": "Init progress UI — real-time step indicator with explanations during project initialization",
      "description": "When a new project initializes, the dashboard shows nothing. Should show: (1) Current step (Scanning codebase... Reading git... Running AI agent...), (2) Progress bar or step counter, (3) What each step does in plain English, (4) Estimated time remaining, (5) Cost estimate. Server already logs steps to console — just need to expose via SSE or polling endpoint and render in the empty state UI.",
      "category": "ux",
      "status": "captured",
      "priority": "high",
      "source": "session-11 dogfood — Pillar initialization",
      "related_ideas": [],
      "promoted_to": null,
      "pros": ["Critical for first-time UX", "User has zero visibility during 3-5 min wait", "Server already logs the steps"],
      "cons": ["Need SSE or polling endpoint for real-time updates"],
      "open_questions": ["SSE stream or polling endpoint?", "Show cost in real-time or just estimate upfront?"],
      "notes": "User literally said 'we should really have progress indicators and explanations of what its doing'",
      "tags": ["ux", "initialization", "onboarding"],
      "created": "2026-02-09",
      "updated": "2026-02-09"
    },
    {
      "id": "IDEA-075",
      "title": "Global credentials — move .credentials.json to ~/.dev-track/credentials.json",
      "description": "Currently API keys live in .credentials.json per project root. When initializing a new project (Pillar), the user has to manually copy credentials. Should be global at ~/.dev-track/credentials.json with per-project override support. Migration: check project root first (backward compat), then fall back to global. Same pattern as global profiles.",
      "category": "architecture",
      "status": "captured",
      "priority": "high",
      "source": "session-11 dogfood — Pillar init failed because no API keys",
      "related_ideas": [],
      "promoted_to": null,
      "pros": ["One-time key setup works across all projects", "Same pattern as global profiles", "Eliminates the #1 new-project friction"],
      "cons": ["Need migration path", "Per-project overrides add complexity"],
      "open_questions": ["Should per-project still be supported as override?"],
      "notes": "User said 'I thought our native dev-track keys were supposed to travel with'",
      "tags": ["architecture", "credentials", "multi-project"],
      "created": "2026-02-09",
      "updated": "2026-02-09"
    },
    {
      "id": "IDEA-076",
      "title": "Background daemon — persistent service for multi-project overnight automations",
      "description": "Current architecture: one Node server per active project, dies when terminal closes. No overnight audits if laptop sleeps. No simultaneous monitoring of multiple projects. Fix: lightweight background daemon (system service) that manages all registered projects. Handles scheduled automations, file watching, and health checks. UI server is just the dashboard that connects to the daemon. Daemon installs via: Windows Task Scheduler, macOS launchd, Linux systemd.",
      "category": "architecture",
      "status": "captured",
      "priority": "medium",
      "source": "session-11 — user identified the multi-project monitoring gap",
      "related_ideas": [],
      "promoted_to": null,
      "pros": ["Overnight audits actually run", "All projects monitored simultaneously", "Survives terminal close / laptop sleep"],
      "cons": ["System service installation is platform-specific", "More complex deployment", "v1.0 feature not MVP"],
      "open_questions": ["Electron app vs native service?", "How to handle laptop sleep/wake?"],
      "notes": "User said 'the more I dig into all this the more im thinking we need a web app with a desktop daemon'",
      "tags": ["architecture", "daemon", "v1.0"],
      "created": "2026-02-09",
      "updated": "2026-02-09"
    }
    ,
    {
      "id": "IDEA-077",
      "title": "Git history import — show commits alongside changelog as a tab or timeline",
      "description": "Import git commit history into DevTrack and display alongside the semantic changelog. Changelog is WHAT shipped and WHY (semantic). Git log is WHAT files changed (mechanical). Both are useful. Show as tabs in Changelog view: 'Changelog | Git History'. Auto-import on session start/end from checkpoint. Could also cross-link: changelog entry CL-090 references commits abc123, def456.",
      "category": "feature",
      "status": "captured",
      "priority": "medium",
      "source": "session-11 — user question about changelog vs git",
      "related_ideas": [],
      "promoted_to": null,
      "pros": ["Full development timeline in one place", "Cross-references between semantic and mechanical changes", "Git history already available via checkpoint system"],
      "cons": ["Could be noisy for large repos", "Need to decide granularity (per-commit vs per-session)"],
      "open_questions": ["Import all history or just per-session?", "Tab in Changelog view or separate view?"],
      "notes": "User asked 'im confused about the difference between our changelog function and githubs'",
      "tags": ["git", "changelog", "timeline"],
      "created": "2026-02-09",
      "updated": "2026-02-09"
    },
    {
      "id": "IDEA-078",
      "title": "Init upfactor — real-time cost/progress streaming, remove iteration limit, Helicone tracking, optimization",
      "description": "The initialization flow needs major upfactoring: (1) Real-time progress streaming via SSE — show current step, entities created so far, running cost, estimated time. (2) Remove the 20-iteration hard limit — let the agent finish the job. (3) Cost display on the loading bar in real-time. (4) Apply learnings from automation refactor (filtered reads, summary mode) where applicable. (5) Wire through Helicone for cost tracking. (6) Better error recovery — if rate limited, wait and continue.",
      "category": "feature",
      "status": "captured",
      "priority": "high",
      "source": "session-11 — Pillar initialization dogfood",
      "related_ideas": ["IDEA-074"],
      "promoted_to": null,
      "pros": ["Critical for first-time UX", "User needs cost visibility", "Current 20-iteration limit is arbitrary"],
      "cons": ["SSE streaming adds complexity", "Uncapped iterations could be expensive"],
      "open_questions": ["Hard cost cap instead of iteration cap?", "Stream via SSE or poll?"],
      "notes": "User: 'I dont want the agent to be limited on the front end initialization really - I want to know what my cost is in real time'",
      "tags": ["initialization", "ux", "streaming", "cost"],
      "created": "2026-02-09",
      "updated": "2026-02-09"
    }
  ],
  "next_id": 79
}
